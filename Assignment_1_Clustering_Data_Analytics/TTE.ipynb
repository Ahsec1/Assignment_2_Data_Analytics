{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup \n",
    "\n",
    "In this section we define our target trial estimands for two scenarios:\n",
    "- **Per-protocol (PP):** Focused on patients adhering strictly to the treatment protocol.\n",
    "- **Intention-to-treat (ITT):** Analyses based on the treatment as assigned regardless of adherence.\n",
    "\n",
    "We also create directories using Pythonâ€™s `tempfile` module to store model outputs or intermediate files for later inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "# Define the estimands\n",
    "estimand_pp = \"PP\"  # Per-protocol\n",
    "estimand_itt = \"ITT\"  # Intention-to-treat\n",
    "\n",
    "# Create directories to save files for later inspection\n",
    "trial_pp_dir = os.path.join(tempfile.gettempdir(), \"trial_pp\")\n",
    "os.makedirs(trial_pp_dir, exist_ok=True)\n",
    "\n",
    "trial_itt_dir = os.path.join(tempfile.gettempdir(), \"trial_itt\")\n",
    "os.makedirs(trial_itt_dir, exist_ok=True)\n",
    "\n",
    "# The data_censored.csv file will be used later for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In this section we load the observational data from the `data_censored.csv` file which will be used for the target trial emulation. \n",
    "The dataset includes columns such as `id`, `period`, `treatment`, `x1`, `x2`, `x3`, `x4`, `age`, `age_s`, `outcome`, `censored`, and `eligible`.\n",
    "\n",
    "We then define a helper function `set_data` to associate specific columns with their roles in the trial data. \n",
    "\n",
    "For the Per-protocol analysis, the dataset is assigned to the `trial_pp` object using a pipe-like style, while for the ITT analysis, a standard function call is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  \\\n",
      "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0   \n",
      "2   1       2          1   0 -0.481762   0  0.734203   38  0.250000        0   \n",
      "3   1       3          1   0  0.007872   0  0.734203   39  0.333333        0   \n",
      "4   1       4          1   1  0.216054   0  0.734203   40  0.416667        0   \n",
      "\n",
      "   censored  eligible  \n",
      "0         0         1  \n",
      "1         0         0  \n",
      "2         0         0  \n",
      "3         0         0  \n",
      "4         0         0  \n"
     ]
    }
   ],
   "source": [
    "# Load the observational data\n",
    "data_censored = pd.read_csv('Data/data_censored.csv')\n",
    "print(data_censored.head())  # display first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial Sequence Object\n",
      "Estimand: Per-protocol\n",
      "\n",
      "Data:\n",
      "  - N: 725 observations from 89 patients\n",
      "         id period treatment    x1           x2   x3        x4   age      age_s\n",
      "      <int> <int>     <num> <num>        <num> <int>     <num> <num>      <num>\n",
      "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
      "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0         0         1\n",
      "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0         0         0\n",
      "---\n",
      "     id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
      "723  99       6          1   1 -0.033762   1  0.575268   71  3.000000        0         0         0\n",
      "724  99       7          0   0 -1.340497   1  0.575268   72  3.083333        1         0         0\n",
      "\n",
      "      outcome censored eligible\n",
      "        <num>    <int>    <num>\n",
      "   outcome  censored  eligible\n",
      "0        0         0         1\n",
      "1        0         0         0\n",
      "---\n",
      "     outcome  censored  eligible\n",
      "723        0         0         0\n",
      "724        1         0         0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Trial Sequence Object\n",
      "Estimand: Intention-to-treat\n",
      "\n",
      "Data:\n",
      "  - N: 725 observations from 89 patients\n",
      "         id period treatment    x1           x2   x3        x4   age      age_s\n",
      "      <int> <int>     <num> <num>        <num> <int>     <num> <num>      <num>\n",
      "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
      "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0         0         1\n",
      "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0         0         0\n",
      "---\n",
      "     id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
      "723  99       6          1   1 -0.033762   1  0.575268   71  3.000000        0         0         0\n",
      "724  99       7          0   0 -1.340497   1  0.575268   72  3.083333        1         0         0\n",
      "\n",
      "      outcome censored eligible\n",
      "        <num>    <int>    <num>\n",
      "   outcome  censored  eligible\n",
      "0        0         0         1\n",
      "1        0         0         0\n",
      "---\n",
      "     outcome  censored  eligible\n",
      "723        0         0         0\n",
      "724        1         0         0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Per-protocol (PP) dataset with data included\n",
    "trial_pp = {\n",
    "    \"data\": data_censored,\n",
    "    \"id\": \"id\",\n",
    "    \"period\": \"period\",\n",
    "    \"treatment\": \"treatment\",\n",
    "    \"outcome\": \"outcome\",\n",
    "    \"eligible\": \"eligible\"\n",
    "}\n",
    "\n",
    "# Define Intention-to-Treat (ITT) dataset with data included\n",
    "trial_itt = {\n",
    "    \"data\": data_censored,\n",
    "    \"id\": \"id\",\n",
    "    \"period\": \"period\",\n",
    "    \"treatment\": \"treatment\",\n",
    "    \"outcome\": \"outcome\",\n",
    "    \"eligible\": \"eligible\"\n",
    "}\n",
    "\n",
    "# Compute total observations and unique patients\n",
    "n_obs = len(data_censored)\n",
    "n_patients = data_censored['id'].nunique()\n",
    "\n",
    "# Get the first 2 rows and last 2 rows of the data\n",
    "head_df = data_censored.head(2)\n",
    "tail_df = data_censored.tail(2)\n",
    "\n",
    "def print_data_showcase(data, estimand_label):\n",
    "    # Compute total observations and unique patients\n",
    "    n_obs = len(data)\n",
    "    n_patients = data['id'].nunique()\n",
    "    \n",
    "    # Get the first 2 rows and last 2 rows of the data\n",
    "    head_df = data.head(2)\n",
    "    tail_df = data.tail(2)\n",
    "    \n",
    "    # Manually construct header strings with column names and types (as in provided example)\n",
    "    print(\"Trial Sequence Object\")\n",
    "    print(\"Estimand: \" + estimand_label)\n",
    "    print(\"\\nData:\")\n",
    "    print(\"  - N: {} observations from {} patients\".format(n_obs, n_patients))\n",
    "    print(\"         id period treatment    x1           x2   x3        x4   age      age_s\")\n",
    "    print(\"      <int> <int>     <num> <num>        <num> <int>     <num> <num>      <num>\")\n",
    "    print(head_df.to_string(index=True))\n",
    "    print(\"---\")\n",
    "    print(tail_df.to_string(index=True))\n",
    "    # For the outcome part, print outcome, censored, eligible columns similarly:\n",
    "    print(\"\\n      outcome censored eligible\")\n",
    "    print(\"        <num>    <int>    <num>\")\n",
    "    head_outcome = head_df[[\"outcome\", \"censored\", \"eligible\"]]\n",
    "    tail_outcome = tail_df[[\"outcome\", \"censored\", \"eligible\"]]\n",
    "    print(head_outcome.to_string(index=True))\n",
    "    print(\"---\")\n",
    "    print(tail_outcome.to_string(index=True))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print showcase for Per-protocol (PP) trial\n",
    "print_data_showcase(trial_pp[\"data\"], \"Per-protocol\")\n",
    "\n",
    "# Print showcase for Intention-to-treat (ITT) trial\n",
    "print_data_showcase(trial_itt[\"data\"], \"Intention-to-treat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Models and Censoring\n",
    "\n",
    "In this step we adjust for informative censoring by applying inverse probability of censoring weights (IPCW). Time-to-event models are constructed to estimate the probability that an observation is not censored, and these probabilities are later used to compute stabilized weights. The configuration of these weight models is stored in the trial objects, while the actual model fitting is deferred until a function such as `calculate_weights()` is invoked.\n",
    "\n",
    "- **Censoring Due to Treatment Switching (PP only):**  \n",
    "  For the Per-protocol estimand, separate models are specified for the numerator (using a limited set of covariates such as age) and the denominator (using an extended set like age, x1, and x3). A dummy model fitter, simulating logistic regression, is used to configure the weight models without immediately fitting them.\n",
    "\n",
    "- **Other Informative Censoring:**  \n",
    "  For both PP and ITT, models are defined to estimate the probability of remaining uncensored. This involves specifying the censoring event (e.g., the \"censored\" column) along with numerator and denominator models (e.g., using x2 in the numerator vs. x2 + x1 in the denominator) and an option to pool models. The configurations are stored, and the models are fit later when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_pp_switch weights config (treatment switching):\n",
      "{'numerator_formula': 'treatment ~ age', 'denominator_formula': 'treatment ~ age + x1 + x3', 'model_fitter': te_stats_glm_logit (save_path=C:\\Users\\USER\\AppData\\Local\\Temp\\trial_pp\\switch_models), 'note': 'Weight models not fitted. Use calculate_weights()'}\n",
      "trial_pp_censor weights config (censoring):\n",
      "{'censor_event': 'censored', 'numerator_formula': '1 - censored ~ x2', 'denominator_formula': '1 - censored ~ x2 + x1', 'pool_models': 'none', 'model_fitter': te_stats_glm_logit (save_path=C:\\Users\\USER\\AppData\\Local\\Temp\\trial_pp\\censor_models), 'note': 'Weight models not fitted. Use calculate_weights()'}\n",
      "trial_itt censor_weights_config:\n",
      "{'censor_event': 'censored', 'numerator_formula': '1 - censored ~ x2', 'denominator_formula': '1 - censored ~ x2 + x1', 'pool_models': 'numerator', 'model_fitter': te_stats_glm_logit (save_path=C:\\Users\\USER\\AppData\\Local\\Temp\\trial_itt\\censor_models), 'note': 'Weight models not fitted. Use calculate_weights()'}\n"
     ]
    }
   ],
   "source": [
    "# Define a dummy model fitter to simulate fitting using logistic regression\n",
    "class StatsGLMLogit:\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "    def __repr__(self):\n",
    "        return f\"te_stats_glm_logit (save_path={self.save_path})\"\n",
    "\n",
    "# Function to set switch weight model (used only for PP)\n",
    "def set_switch_weight_model(trial, numerator, denominator, model_fitter):\n",
    "    trial[\"switch_weights_config\"] = {\n",
    "         \"numerator_formula\": f\"treatment ~ {numerator}\",\n",
    "         \"denominator_formula\": f\"treatment ~ {denominator}\",\n",
    "         \"model_fitter\": model_fitter,\n",
    "         \"note\": \"Weight models not fitted. Use calculate_weights()\"\n",
    "    }\n",
    "    return trial\n",
    "\n",
    "# Function to set censor weight model for informative censoring\n",
    "def set_censor_weight_model(trial, censor_event, numerator, denominator, pool_models, model_fitter):\n",
    "    trial[\"censor_weights_config\"] = {\n",
    "         \"censor_event\": censor_event,\n",
    "         \"numerator_formula\": f\"1 - {censor_event} ~ {numerator}\",\n",
    "         \"denominator_formula\": f\"1 - {censor_event} ~ {denominator}\",\n",
    "         \"pool_models\": pool_models,\n",
    "         \"model_fitter\": model_fitter,\n",
    "         \"note\": \"Weight models not fitted. Use calculate_weights()\"\n",
    "    }\n",
    "    return trial\n",
    "\n",
    "# Apply treatment switching weight model for PP and assign to a distinct variable.\n",
    "trial_pp_switch = set_switch_weight_model(\n",
    "    trial_pp,\n",
    "    numerator=\"age\",\n",
    "    denominator=\"age + x1 + x3\",\n",
    "    model_fitter=StatsGLMLogit(save_path=os.path.join(trial_pp_dir, \"switch_models\"))\n",
    ")\n",
    "print(\"trial_pp_switch weights config (treatment switching):\")\n",
    "print(trial_pp_switch[\"switch_weights_config\"])\n",
    "\n",
    "# Apply censor weight model on a copy of PP to keep it separate.\n",
    "trial_pp_censor = set_censor_weight_model(\n",
    "    trial_pp.copy(),\n",
    "    censor_event=\"censored\",\n",
    "    numerator=\"x2\",\n",
    "    denominator=\"x2 + x1\",\n",
    "    pool_models=\"none\",\n",
    "    model_fitter=StatsGLMLogit(save_path=os.path.join(trial_pp_dir, \"censor_models\"))\n",
    ")\n",
    "print(\"trial_pp_censor weights config (censoring):\")\n",
    "print(trial_pp_censor[\"censor_weights_config\"])\n",
    "\n",
    "# For ITT, censoring weights remain as before.\n",
    "trial_itt = set_censor_weight_model(\n",
    "    trial_itt,\n",
    "    censor_event=\"censored\",\n",
    "    numerator=\"x2\",\n",
    "    denominator=\"x2 + x1\",\n",
    "    pool_models=\"numerator\",\n",
    "    model_fitter=StatsGLMLogit(save_path=os.path.join(trial_itt_dir, \"censor_models\"))\n",
    ")\n",
    "print(\"trial_itt censor_weights_config:\")\n",
    "print(trial_itt[\"censor_weights_config\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Weights\n",
    "\n",
    "In this step we fit the individual models that were configured in Step 3 and then combine them into inverse probability of censoring weights (IPCW). The function `calculate_weights()` is used to perform the model fitting. The fitted model objects are saved on disk in the directories we created earlier, and the weight model summaries are stored in the trial sequence objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored ITT data with calculated weights to CSV file at:\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\\trial_itt_data_with_weights.csv\n",
      "\n",
      "Model n (Numerator) Summary:\n",
      "                         Results: Logit\n",
      "================================================================\n",
      "Model:              Logit            Method:           MLE      \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.027    \n",
      "Date:               2025-03-09 15:33 AIC:              397.4004 \n",
      "No. Observations:   725              BIC:              406.5727 \n",
      "Df Model:           1                Log-Likelihood:   -196.70  \n",
      "Df Residuals:       723              LL-Null:          -202.11  \n",
      "Converged:          1.0000           LLR p-value:      0.0010067\n",
      "No. Iterations:     7.0000           Scale:            1.0000   \n",
      "-----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "-----------------------------------------------------------------\n",
      "Intercept     2.4481    0.1406  17.4149  0.0000   2.1726   2.7236\n",
      "x2           -0.4486    0.1369  -3.2777  0.0010  -0.7169  -0.1804\n",
      "================================================================\n",
      "\n",
      "\n",
      "Model d0 (Denom. for prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.066     \n",
      "Date:               2025-03-09 15:33 AIC:              270.3309  \n",
      "No. Observations:   426              BIC:              282.4943  \n",
      "Df Model:           2                Log-Likelihood:   -132.17   \n",
      "Df Residuals:       423              LL-Null:          -141.54   \n",
      "Converged:          1.0000           LLR p-value:      8.5186e-05\n",
      "No. Iterations:     7.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      1.8942    0.2071   9.1457  0.0000   1.4883   2.3001\n",
      "x2            -0.5898    0.1693  -3.4831  0.0005  -0.9217  -0.2579\n",
      "x1             0.8553    0.3453   2.4769  0.0133   0.1785   1.5320\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Model d1 (Denom. for prev_treatment = 1) Summary:\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.014   \n",
      "Date:               2025-03-09 15:33 AIC:              117.4588\n",
      "No. Observations:   299              BIC:              128.5601\n",
      "Df Model:           2                Log-Likelihood:   -55.729 \n",
      "Df Residuals:       296              LL-Null:          -56.526 \n",
      "Converged:          1.0000           LLR p-value:      0.45067 \n",
      "No. Iterations:     8.0000           Scale:            1.0000  \n",
      "----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025  0.975]\n",
      "----------------------------------------------------------------\n",
      "Intercept     2.8144    0.3123   9.0129  0.0000   2.2024  3.4265\n",
      "x2           -0.0371    0.2700  -0.1375  0.8906  -0.5662  0.4920\n",
      "x1            0.8935    0.7772   1.1496  0.2503  -0.6298  2.4168\n",
      "===============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import joblib\n",
    "\n",
    "def calculate_itt_weights(trial):\n",
    "    # Work with the full dataset (725 observations)\n",
    "    data = trial[\"data\"].copy()\n",
    "    data = data.sort_values([\"id\", \"period\"])\n",
    "    data[\"prev_treatment\"] = data.groupby(\"id\")[\"treatment\"].shift(1).fillna(0)\n",
    "    data[\"not_censored\"] = 1 - data[\"censored\"]\n",
    "    \n",
    "    # Model n: P(censor_event = 0 | X)\n",
    "    formula_n = \"not_censored ~ x2\"\n",
    "    model_n = smf.logit(formula=formula_n, data=data).fit(disp=0)\n",
    "    trial[\"fitted_itt_censor_numerator\"] = model_n\n",
    "    \n",
    "    # Model d0: P(censor_event = 0 | X, previous treatment = 0)\n",
    "    subset_d0 = data[data[\"prev_treatment\"] == 0]\n",
    "    formula_d = \"not_censored ~ x2 + x1\"\n",
    "    model_d0 = smf.logit(formula=formula_d, data=subset_d0).fit(disp=0)\n",
    "    trial[\"fitted_itt_censor_denominator_d0\"] = model_d0\n",
    "    \n",
    "    # Model d1: P(censor_event = 0 | X, previous treatment = 1)\n",
    "    subset_d1 = data[data[\"prev_treatment\"] == 1]\n",
    "    model_d1 = smf.logit(formula=formula_d, data=subset_d1).fit(disp=0)\n",
    "    trial[\"fitted_itt_censor_denominator_d1\"] = model_d1\n",
    "    \n",
    "    # Compute predicted probabilities for all observations\n",
    "    data[\"pred_num\"] = model_n.predict(data)\n",
    "    data[\"pred_den\"] = 0.0\n",
    "    idx0 = data[\"prev_treatment\"] == 0\n",
    "    idx1 = data[\"prev_treatment\"] == 1\n",
    "    data.loc[idx0, \"pred_den\"] = model_d0.predict(data.loc[idx0])\n",
    "    data.loc[idx1, \"pred_den\"] = model_d1.predict(data.loc[idx1])\n",
    "    \n",
    "    # Calculate weight as the ratio of predicted numerator to denominator\n",
    "    data[\"weight\"] = data[\"pred_num\"] / data[\"pred_den\"]\n",
    "    \n",
    "    # Save the full dataset with calculated weights\n",
    "    trial[\"data_with_weights\"] = data.copy()\n",
    "    \n",
    "    return trial\n",
    "\n",
    "# --- Compute ITT weights ---\n",
    "trial_itt = calculate_itt_weights(trial_itt)\n",
    "\n",
    "# --- Define the path to save the CSV file ---\n",
    "data_folder = r\"C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\"\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "csv_path = os.path.join(data_folder, \"trial_itt_data_with_weights.csv\")\n",
    "\n",
    "# --- Save the data with weights ---\n",
    "trial_itt[\"data_with_weights\"].to_csv(csv_path, index=False)\n",
    "print(\"Stored ITT data with calculated weights to CSV file at:\")\n",
    "print(csv_path)\n",
    "\n",
    "# --- Print the fitted models' summaries ---\n",
    "print(\"\\nModel n (Numerator) Summary:\")\n",
    "print(trial_itt[\"fitted_itt_censor_numerator\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nModel d0 (Denom. for prev_treatment = 0) Summary:\")\n",
    "print(trial_itt[\"fitted_itt_censor_denominator_d0\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nModel d1 (Denom. for prev_treatment = 1) Summary:\")\n",
    "print(trial_itt[\"fitted_itt_censor_denominator_d1\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored PP data with calculated weights to CSV file at:\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\\trial_pp_data_with_weights.csv\n",
      "\n",
      "PP model n0 (Numerator, prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.043     \n",
      "Date:               2025-03-09 15:33 AIC:              274.8722  \n",
      "No. Observations:   426              BIC:              282.9811  \n",
      "Df Model:           1                Log-Likelihood:   -135.44   \n",
      "Df Residuals:       424              LL-Null:          -141.54   \n",
      "Converged:          1.0000           LLR p-value:      0.00047787\n",
      "No. Iterations:     7.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      2.2450    0.1718  13.0698  0.0000   1.9083   2.5817\n",
      "x2            -0.5739    0.1670  -3.4366  0.0006  -0.9013  -0.2466\n",
      "=================================================================\n",
      "\n",
      "\n",
      "PP model n1 (Numerator, prev_treatment = 1) Summary:\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.000   \n",
      "Date:               2025-03-09 15:33 AIC:              117.0524\n",
      "No. Observations:   299              BIC:              124.4533\n",
      "Df Model:           1                Log-Likelihood:   -56.526 \n",
      "Df Residuals:       297              LL-Null:          -56.526 \n",
      "Converged:          1.0000           LLR p-value:      0.98321 \n",
      "No. Iterations:     7.0000           Scale:            1.0000  \n",
      "----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025  0.975]\n",
      "----------------------------------------------------------------\n",
      "Intercept     3.0116    0.2873  10.4809  0.0000   2.4484  3.5748\n",
      "x2           -0.0057    0.2705  -0.0210  0.9832  -0.5359  0.5245\n",
      "===============================================================\n",
      "\n",
      "\n",
      "PP model d0 (Denominator, prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.066     \n",
      "Date:               2025-03-09 15:33 AIC:              270.3309  \n",
      "No. Observations:   426              BIC:              282.4943  \n",
      "Df Model:           2                Log-Likelihood:   -132.17   \n",
      "Df Residuals:       423              LL-Null:          -141.54   \n",
      "Converged:          1.0000           LLR p-value:      8.5186e-05\n",
      "No. Iterations:     7.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      1.8942    0.2071   9.1457  0.0000   1.4883   2.3001\n",
      "x2            -0.5898    0.1693  -3.4831  0.0005  -0.9217  -0.2579\n",
      "x1             0.8553    0.3453   2.4769  0.0133   0.1785   1.5320\n",
      "=================================================================\n",
      "\n",
      "\n",
      "PP model d1 (Denominator, prev_treatment = 1) Summary:\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.014   \n",
      "Date:               2025-03-09 15:33 AIC:              117.4588\n",
      "No. Observations:   299              BIC:              128.5601\n",
      "Df Model:           2                Log-Likelihood:   -55.729 \n",
      "Df Residuals:       296              LL-Null:          -56.526 \n",
      "Converged:          1.0000           LLR p-value:      0.45067 \n",
      "No. Iterations:     8.0000           Scale:            1.0000  \n",
      "----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025  0.975]\n",
      "----------------------------------------------------------------\n",
      "Intercept     2.8144    0.3123   9.0129  0.0000   2.2024  3.4265\n",
      "x2           -0.0371    0.2700  -0.1375  0.8906  -0.5662  0.4920\n",
      "x1            0.8935    0.7772   1.1496  0.2503  -0.6298  2.4168\n",
      "===============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import joblib\n",
    "\n",
    "def calculate_pp_informative_weights_updated(trial):\n",
    "    data = trial[\"data\"].copy()\n",
    "    data = data.sort_values([\"id\", \"period\"])\n",
    "    data[\"prev_treatment\"] = data.groupby(\"id\")[\"treatment\"].shift(1).fillna(0)\n",
    "    data[\"not_censored\"] = 1 - data[\"censored\"]\n",
    "    \n",
    "    # Model n0: P(censor_event = 0 | X, previous treatment = 0) for numerator\n",
    "    subset0 = data[data[\"prev_treatment\"] == 0]\n",
    "    model_n0 = smf.logit(\"not_censored ~ x2\", data=subset0).fit(disp=0)\n",
    "    trial[\"fitted_pp_censor_numerator_n0\"] = model_n0\n",
    "\n",
    "    # Model n1: P(censor_event = 0 | X, previous treatment = 1) for numerator\n",
    "    subset1 = data[data[\"prev_treatment\"] == 1]\n",
    "    model_n1 = smf.logit(\"not_censored ~ x2\", data=subset1).fit(disp=0)\n",
    "    trial[\"fitted_pp_censor_numerator_n1\"] = model_n1\n",
    "\n",
    "    # Model d0: P(censor_event = 0 | X, previous treatment = 0) for denominator\n",
    "    model_d0 = smf.logit(\"not_censored ~ x2 + x1\", data=subset0).fit(disp=0)\n",
    "    trial[\"fitted_pp_censor_denominator_d0\"] = model_d0\n",
    "\n",
    "    # Model d1: P(censor_event = 0 | X, previous treatment = 1) for denominator\n",
    "    model_d1 = smf.logit(\"not_censored ~ x2 + x1\", data=subset1).fit(disp=0)\n",
    "    trial[\"fitted_pp_censor_denominator_d1\"] = model_d1\n",
    "\n",
    "    # Compute predicted probabilities\n",
    "    data[\"pred_num\"] = 0.0\n",
    "    data[\"pred_den\"] = 0.0\n",
    "    idx0 = data[\"prev_treatment\"] == 0\n",
    "    idx1 = data[\"prev_treatment\"] == 1\n",
    "\n",
    "    data.loc[idx0, \"pred_num\"] = model_n0.predict(data.loc[idx0])\n",
    "    data.loc[idx1, \"pred_num\"] = model_n1.predict(data.loc[idx1])\n",
    "    data.loc[idx0, \"pred_den\"] = model_d0.predict(data.loc[idx0])\n",
    "    data.loc[idx1, \"pred_den\"] = model_d1.predict(data.loc[idx1])\n",
    "\n",
    "    # Calculate weight as the ratio of numerator to denominator predictions\n",
    "    data[\"weight\"] = data[\"pred_num\"] / data[\"pred_den\"]\n",
    "\n",
    "    # Save the full dataset with calculated weights\n",
    "    trial[\"data_with_weights\"] = data.copy()\n",
    "\n",
    "    return trial\n",
    "\n",
    "# --- Compute PP weights ---\n",
    "trial_pp_censor[\"estimand\"] = \"PP\"\n",
    "trial_pp_censor[\"save_dir\"] = os.path.join(trial_pp_dir, \"informative_censor_models\")\n",
    "os.makedirs(trial_pp_censor[\"save_dir\"], exist_ok=True)\n",
    "trial_pp_censor = calculate_pp_informative_weights_updated(trial_pp_censor)\n",
    "\n",
    "# --- Define the path to save the CSV file ---\n",
    "data_folder = r\"C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\"\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "csv_path = os.path.join(data_folder, \"trial_pp_data_with_weights.csv\")\n",
    "\n",
    "# --- Save the data with weights ---\n",
    "trial_pp_censor[\"data_with_weights\"].to_csv(csv_path, index=False)\n",
    "print(\"Stored PP data with calculated weights to CSV file at:\")\n",
    "print(csv_path)\n",
    "\n",
    "# --- Print the fitted models' summaries ---\n",
    "print(\"\\nPP model n0 (Numerator, prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_numerator_n0\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nPP model n1 (Numerator, prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_numerator_n1\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nPP model d0 (Denominator, prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_denominator_d0\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nPP model d1 (Denominator, prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_denominator_d1\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment Switching Weight Calculation\n",
    "\n",
    "In this section, we calculate the treatment switching weights for the Per-protocol (PP) analysis.\n",
    "We train four logistic regression models:\n",
    "- model n1: P(treatment = 1 | previous treatment = 1) for numerator.\n",
    "- model d1: P(treatment = 1 | previous treatment = 1) for denominator.\n",
    "- model n0: P(treatment = 1 | previous treatment = 0) for numerator.\n",
    "- model d0: P(treatment = 1 | previous treatment = 0) for denominator.\n",
    "\n",
    "The weights are calculated as follows:\n",
    "- For observations with previous treatment = 1:\n",
    "    weight = (predicted probability from model n1) / (predicted probability from model d1)\n",
    "- For observations with previous treatment = 0:\n",
    "    weight = (predicted probability from model n0) / (predicted probability from model d0)\n",
    "\n",
    "Logistic regression is used to estimate these probabilities and the models are saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model n1 (Numerator for prev_treatment = 1) Summary:\n",
      "                         Results: Logit\n",
      "================================================================\n",
      "Model:              Logit            Method:           MLE      \n",
      "Dependent Variable: treatment        Pseudo R-squared: 0.021    \n",
      "Date:               2025-03-09 15:33 AIC:              386.9911 \n",
      "No. Observations:   299              BIC:              394.3920 \n",
      "Df Model:           1                Log-Likelihood:   -191.50  \n",
      "Df Residuals:       297              LL-Null:          -195.58  \n",
      "Converged:          1.0000           LLR p-value:      0.0042698\n",
      "No. Iterations:     5.0000           Scale:            1.0000   \n",
      "-----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "-----------------------------------------------------------------\n",
      "Intercept     2.0396    0.5421   3.7625  0.0002   0.9771   3.1021\n",
      "age          -0.0311    0.0110  -2.8112  0.0049  -0.0527  -0.0094\n",
      "================================================================\n",
      "\n",
      "\n",
      "Model d1 (Denom. for prev_treatment = 1) Summary:\n",
      "                         Results: Logit\n",
      "================================================================\n",
      "Model:              Logit            Method:           MLE      \n",
      "Dependent Variable: treatment        Pseudo R-squared: 0.035    \n",
      "Date:               2025-03-09 15:33 AIC:              385.3454 \n",
      "No. Observations:   299              BIC:              400.1472 \n",
      "Df Model:           3                Log-Likelihood:   -188.67  \n",
      "Df Residuals:       295              LL-Null:          -195.58  \n",
      "Converged:          1.0000           LLR p-value:      0.0031740\n",
      "No. Iterations:     5.0000           Scale:            1.0000   \n",
      "-----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "-----------------------------------------------------------------\n",
      "Intercept     1.7547    0.5860   2.9942  0.0028   0.6061   2.9033\n",
      "age          -0.0303    0.0113  -2.6857  0.0072  -0.0524  -0.0082\n",
      "x1            0.6544    0.2892   2.2631  0.0236   0.0877   1.2211\n",
      "x3            0.1615    0.2502   0.6456  0.5186  -0.3288   0.6518\n",
      "================================================================\n",
      "\n",
      "\n",
      "Model n0 (Numerator for prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: treatment        Pseudo R-squared: 0.052     \n",
      "Date:               2025-03-09 15:33 AIC:              525.6219  \n",
      "No. Observations:   426              BIC:              533.7307  \n",
      "Df Model:           1                Log-Likelihood:   -260.81   \n",
      "Df Residuals:       424              LL-Null:          -275.13   \n",
      "Converged:          1.0000           LLR p-value:      8.7692e-08\n",
      "No. Iterations:     5.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      1.5949    0.4381   3.6405  0.0003   0.7362   2.4535\n",
      "age           -0.0463    0.0090  -5.1477  0.0000  -0.0639  -0.0287\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Model d0 (Denom. for prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: treatment        Pseudo R-squared: 0.067     \n",
      "Date:               2025-03-09 15:33 AIC:              521.3314  \n",
      "No. Observations:   426              BIC:              537.5492  \n",
      "Df Model:           3                Log-Likelihood:   -256.67   \n",
      "Df Residuals:       422              LL-Null:          -275.13   \n",
      "Converged:          1.0000           LLR p-value:      4.7872e-08\n",
      "No. Iterations:     5.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      1.4907    0.4708   3.1662  0.0015   0.5679   2.4135\n",
      "age           -0.0491    0.0092  -5.3321  0.0000  -0.0672  -0.0311\n",
      "x1             0.5951    0.2150   2.7686  0.0056   0.1738   1.0164\n",
      "x3            -0.1393    0.2141  -0.6506  0.5153  -0.5590   0.2804\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New Code Cell: Calculate Treatment Switching Weights using Logistic Regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def calculate_pp_switch_weights(trial):\n",
    "    # Ensure trial is PP and prepare data\n",
    "    data = trial[\"data\"].copy()\n",
    "    data = data.sort_values([\"id\", \"period\"])\n",
    "    data[\"prev_treatment\"] = data.groupby(\"id\")[\"treatment\"].shift(1).fillna(0)\n",
    "    \n",
    "    # Model n1: P(treatment = 1 | previous treatment = 1)\n",
    "    subset_n1 = data[data[\"prev_treatment\"] == 1]\n",
    "    formula_n1 = \"treatment ~ age\"\n",
    "    model_n1 = smf.logit(formula=formula_n1, data=subset_n1).fit(disp=0)\n",
    "    save_path_n1 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_num_model_n1.pkl\")\n",
    "    joblib.dump(model_n1, save_path_n1)\n",
    "    trial[\"fitted_pp_switch_numerator_n1\"] = model_n1\n",
    "    \n",
    "    # Model d1: P(treatment = 1 | previous treatment = 1)\n",
    "    formula_d1 = \"treatment ~ age + x1 + x3\"\n",
    "    model_d1 = smf.logit(formula=formula_d1, data=subset_n1).fit(disp=0)\n",
    "    save_path_d1 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_den_model_d1.pkl\")\n",
    "    joblib.dump(model_d1, save_path_d1)\n",
    "    trial[\"fitted_pp_switch_denominator_d1\"] = model_d1\n",
    "    \n",
    "    # Model n0: P(treatment = 1 | previous treatment = 0)\n",
    "    subset_n0 = data[data[\"prev_treatment\"] == 0]\n",
    "    formula_n0 = \"treatment ~ age\"\n",
    "    model_n0 = smf.logit(formula=formula_n0, data=subset_n0).fit(disp=0)\n",
    "    save_path_n0 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_num_model_n0.pkl\")\n",
    "    joblib.dump(model_n0, save_path_n0)\n",
    "    trial[\"fitted_pp_switch_numerator_n0\"] = model_n0\n",
    "    \n",
    "    # Model d0: P(treatment = 1 | previous treatment = 0)\n",
    "    formula_d0 = \"treatment ~ age + x1 + x3\"\n",
    "    model_d0 = smf.logit(formula=formula_d0, data=subset_n0).fit(disp=0)\n",
    "    save_path_d0 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_den_model_d0.pkl\")\n",
    "    joblib.dump(model_d0, save_path_d0)\n",
    "    trial[\"fitted_pp_switch_denominator_d0\"] = model_d0\n",
    "    \n",
    "    return trial\n",
    "\n",
    "# Usage example for trial PP:\n",
    "# Ensure trial_pp has an assigned save_dir (e.g., within trial_pp_dir)\n",
    "trial_pp_switch[\"estimand\"] = \"PP\"\n",
    "trial_pp_switch[\"save_dir\"] = os.path.join(trial_pp_dir, \"switch_models\")\n",
    "os.makedirs(trial_pp_switch[\"save_dir\"], exist_ok=True)\n",
    "trial_pp_switch = calculate_pp_switch_weights(trial_pp_switch)\n",
    "\n",
    "# To verify, you can print summaries:\n",
    "print(\"Model n1 (Numerator for prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_numerator_n1\"].summary2().as_text())\n",
    "print(\"\\nModel d1 (Denom. for prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_denominator_d1\"].summary2().as_text())\n",
    "print(\"\\nModel n0 (Numerator for prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_numerator_n0\"].summary2().as_text())\n",
    "print(\"\\nModel d0 (Denom. for prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_denominator_d0\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Outcome Model\n",
    "Now we can specify the outcome model. Here we can include adjustment terms for any variables in the dataset. The numerator terms from the stabilised weight models are automatically included in the outcome model formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP Outcome Model Summary:\n",
      "                 Results: Ordinary least squares\n",
      "==================================================================\n",
      "Model:              OLS              Adj. R-squared:     0.001    \n",
      "Dependent Variable: outcome          AIC:                -987.8294\n",
      "Date:               2025-03-09 15:33 BIC:                -978.6570\n",
      "No. Observations:   725              Log-Likelihood:     495.91   \n",
      "Df Model:           1                F-statistic:        1.798    \n",
      "Df Residuals:       723              Prob (F-statistic): 0.180    \n",
      "R-squared:          0.002            Scale:              0.014948 \n",
      "--------------------------------------------------------------------\n",
      "             Coef.    Std.Err.      z      P>|z|     [0.025   0.975]\n",
      "--------------------------------------------------------------------\n",
      "Intercept    0.0207     0.0073    2.8582   0.0043    0.0065   0.0349\n",
      "treatment   -0.0119     0.0089   -1.3408   0.1800   -0.0292   0.0055\n",
      "------------------------------------------------------------------\n",
      "Omnibus:             1016.550     Durbin-Watson:        1.938     \n",
      "Prob(Omnibus):       0.000        Jarque-Bera (JB):     118616.404\n",
      "Skew:                7.904        Prob(JB):             0.000     \n",
      "Kurtosis:            63.636       Condition No.:        3         \n",
      "==================================================================\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
      "\n",
      "ITT Outcome Model Summary:\n",
      "                 Results: Weighted least squares\n",
      "==================================================================\n",
      "Model:              WLS              Adj. R-squared:     0.000    \n",
      "Dependent Variable: outcome          AIC:                -980.2986\n",
      "Date:               2025-03-09 15:33 BIC:                -966.5400\n",
      "No. Observations:   725              Log-Likelihood:     493.15   \n",
      "Df Model:           2                F-statistic:        1.143    \n",
      "Df Residuals:       722              Prob (F-statistic): 0.320    \n",
      "R-squared:          0.003            Scale:              0.015108 \n",
      "--------------------------------------------------------------------\n",
      "             Coef.    Std.Err.      z      P>|z|     [0.025   0.975]\n",
      "--------------------------------------------------------------------\n",
      "Intercept    0.0217     0.0076    2.8667   0.0041    0.0069   0.0366\n",
      "treatment   -0.0126     0.0090   -1.3999   0.1616   -0.0303   0.0050\n",
      "x2           0.0036     0.0046    0.7882   0.4306   -0.0054   0.0127\n",
      "------------------------------------------------------------------\n",
      "Omnibus:             1016.372     Durbin-Watson:        1.938     \n",
      "Prob(Omnibus):       0.000        Jarque-Bera (JB):     118763.267\n",
      "Skew:                7.901        Prob(JB):             0.000     \n",
      "Kurtosis:            63.678       Condition No.:        3         \n",
      "==================================================================\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC0)\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def set_outcome_model(trial, adjustment_terms=\"\"):\n",
    "    # Use data with weights if available; otherwise, use the original data.\n",
    "    data = trial.get(\"data_with_weights\", trial[\"data\"]).copy()\n",
    "    \n",
    "    # Ensure adjustment terms are formatted correctly\n",
    "    formula = f\"outcome ~ treatment\"\n",
    "    if adjustment_terms:\n",
    "        formula += f\" {adjustment_terms}\"  # Adds adjustment terms only if provided\n",
    "    \n",
    "    # Fit the model: Weighted if weights exist, otherwise standard OLS\n",
    "    if \"weight\" in data.columns:\n",
    "        model = smf.wls(formula=formula, data=data, weights=data[\"weight\"]).fit(cov_type=\"HC0\")\n",
    "    else:\n",
    "        model = smf.ols(formula=formula, data=data).fit(cov_type=\"HC0\")\n",
    "    \n",
    "    trial[\"outcome_model\"] = model\n",
    "    return trial\n",
    "\n",
    "# --- Apply outcome model for PP and ITT using the data that already has calculated weights\n",
    "trial_pp = set_outcome_model(trial_pp)\n",
    "trial_itt = set_outcome_model(trial_itt, adjustment_terms=\" + x2\")\n",
    "\n",
    "# --- Print the outcome model summaries\n",
    "print(\"PP Outcome Model Summary:\")\n",
    "print(trial_pp[\"outcome_model\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nITT Outcome Model Summary:\")\n",
    "print(trial_itt[\"outcome_model\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand Trials\n",
    "\n",
    "We prepare to create the dataset that includes the sequence of target trials. This involves expanding the trial data to include all possible sequences of treatment and control assignments for each patient. \n",
    "\n",
    "We use the `set_expansion_options` function to configure the expansion process. This function allows us to specify the output method and the chunk size, which determines the number of patients to include in each expansion iteration. \n",
    "\n",
    "For both the Per-protocol (PP) and Intention-to-treat (ITT) analyses, we set the output to a dummy function `save_to_datatable()` and the chunk size to 500 patients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored PP expanded data at: C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\\trial_pp_expanded.csv\n",
      "Stored ITT expanded data at: C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\\trial_itt_expanded.csv\n",
      "\n",
      "Sample from PP Expanded Data:\n",
      "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  \\\n",
      "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "1   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "2   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "3   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "4   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "\n",
      "   censored  eligible  prev_treatment  not_censored  pred_num  pred_den  \\\n",
      "0         0         1             0.0             1   0.83022  0.888293   \n",
      "1         0         1             0.0             1   0.83022  0.888293   \n",
      "2         0         1             0.0             1   0.83022  0.888293   \n",
      "3         0         1             0.0             1   0.83022  0.888293   \n",
      "4         0         1             0.0             1   0.83022  0.888293   \n",
      "\n",
      "     weight  trial_period  followup_time  \n",
      "0  0.934624             0              0  \n",
      "1  0.934624             1              1  \n",
      "2  0.934624             2              2  \n",
      "3  0.934624             3              3  \n",
      "4  0.934624             4              4  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def expand_trials(data, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Expands trial data in chunks, repeating each row for multiple trial periods.\n",
    "    \"\"\"\n",
    "    expanded_chunks = []\n",
    "\n",
    "    # Define trial periods (consistent with tutorial defaults)\n",
    "    periods = np.arange(11)  # Includes periods 0 to 10\n",
    "\n",
    "    # Process data in chunks\n",
    "    for start in range(0, len(data), chunk_size):\n",
    "        chunk = data.iloc[start:start + chunk_size].copy()\n",
    "\n",
    "        expanded_chunk = (\n",
    "            chunk.loc[chunk.index.repeat(len(periods))]  # Repeat rows for each period\n",
    "            .assign(trial_period=np.tile(periods, len(chunk)))  # Assign trial periods\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Assign follow-up time (same as trial_period)\n",
    "        expanded_chunk[\"followup_time\"] = expanded_chunk[\"trial_period\"]\n",
    "\n",
    "        expanded_chunks.append(expanded_chunk)\n",
    "\n",
    "    # Concatenate all expanded chunks\n",
    "    return pd.concat(expanded_chunks, ignore_index=True)\n",
    "\n",
    "# --- Expand both PP and ITT datasets ---\n",
    "trial_pp_expanded = expand_trials(trial_pp_censor[\"data_with_weights\"], chunk_size=500)\n",
    "trial_itt_expanded = expand_trials(trial_itt[\"data_with_weights\"], chunk_size=500)\n",
    "\n",
    "# --- Define the path to save the CSV files ---\n",
    "data_folder = r\"C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\"\n",
    "os.makedirs(data_folder, exist_ok=True)  # Ensure the folder exists\n",
    "\n",
    "# --- Save expanded datasets ---\n",
    "csv_path_pp = os.path.join(data_folder, \"trial_pp_expanded.csv\")\n",
    "csv_path_itt = os.path.join(data_folder, \"trial_itt_expanded.csv\")\n",
    "\n",
    "trial_pp_expanded.to_csv(csv_path_pp, index=False)\n",
    "trial_itt_expanded.to_csv(csv_path_itt, index=False)\n",
    "\n",
    "print(\"Stored PP expanded data at:\", csv_path_pp)\n",
    "print(\"Stored ITT expanded data at:\", csv_path_itt)\n",
    "\n",
    "# --- Check sample output ---\n",
    "print(\"\\nSample from PP Expanded Data:\")\n",
    "print(trial_pp_expanded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load or Sample from Expanded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored sampled ITT data at: C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\\trial_itt_load.csv\n",
      "\n",
      "Sample from ITT Sampled Data:\n",
      "    id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  \\\n",
      "0    1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "2    1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "5    1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "6    1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "10   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "\n",
      "    censored  eligible  prev_treatment  not_censored  pred_num  pred_den  \\\n",
      "0          0         1             0.0             1  0.873678  0.888293   \n",
      "2          0         1             0.0             1  0.873678  0.888293   \n",
      "5          0         1             0.0             1  0.873678  0.888293   \n",
      "6          0         1             0.0             1  0.873678  0.888293   \n",
      "10         0         1             0.0             1  0.873678  0.888293   \n",
      "\n",
      "      weight  trial_period  followup_time  \n",
      "0   0.983546             0              0  \n",
      "2   0.983546             2              2  \n",
      "5   0.983546             5              5  \n",
      "6   0.983546             6              6  \n",
      "10  0.983546            10             10  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_expanded_data(trial, seed=1234, p_control=0.5, period_range=None, subset_condition=None):\n",
    "    \"\"\"\n",
    "    Loads and samples from expanded trial data.\n",
    "    \n",
    "    Parameters:\n",
    "        trial (dict): Trial dictionary containing expanded trial data.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        p_control (float): Probability of including outcome == 0 observations.\n",
    "        period_range (tuple): (min_period, max_period) to filter specific periods.\n",
    "        subset_condition (str): Pandas query condition to filter rows.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered and sampled expanded dataset.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)  # Set seed for reproducibility\n",
    "\n",
    "    # Load the expanded data\n",
    "    data = trial[\"expanded_data\"].copy()\n",
    "\n",
    "    # Apply period filtering if specified\n",
    "    if period_range:\n",
    "        min_period, max_period = period_range\n",
    "        data = data[(data[\"trial_period\"] >= min_period) & (data[\"trial_period\"] <= max_period)]\n",
    "\n",
    "    # Apply subset condition if specified (e.g., \"age > 65\")\n",
    "    if subset_condition:\n",
    "        data = data.query(subset_condition)\n",
    "\n",
    "    # Apply p_control sampling: Keep all outcome == 1, sample outcome == 0\n",
    "    outcome_0_mask = (data[\"outcome\"] == 0)\n",
    "    sampled_data = data.loc[~outcome_0_mask | (np.random.rand(len(data)) < p_control)]\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "# --- Load and sample from expanded ITT data ---\n",
    "trial_itt[\"expanded_data\"] = trial_itt_expanded  # Ensure expanded data is stored in trial_itt\n",
    "sampled_itt_data = load_expanded_data(trial_itt, seed=1234, p_control=0.5)\n",
    "\n",
    "# --- Define the path to save the sampled data ---\n",
    "data_folder = r\"C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\"\n",
    "csv_path_loead = os.path.join(data_folder, \"trial_itt_load.csv\")\n",
    "\n",
    "# --- Save the sampled dataset ---\n",
    "sampled_itt_data.to_csv(csv_path_sampled, index=False)\n",
    "\n",
    "print(\"Stored sampled ITT data at:\", csv_path_loead)\n",
    "\n",
    "# --- Check sample output ---\n",
    "print(\"\\nSample from ITT Sampled Data:\")\n",
    "print(sampled_itt_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
