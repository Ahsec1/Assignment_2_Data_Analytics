{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup \n",
    "\n",
    "In this section we define our target trial estimands for two scenarios:\n",
    "- **Per-protocol (PP):** Focused on patients adhering strictly to the treatment protocol.\n",
    "- **Intention-to-treat (ITT):** Analyses based on the treatment as assigned regardless of adherence.\n",
    "\n",
    "We also create directories using Pythonâ€™s `tempfile` module to store model outputs or intermediate files for later inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "# Define the estimands\n",
    "estimand_pp = \"PP\"  # Per-protocol\n",
    "estimand_itt = \"ITT\"  # Intention-to-treat\n",
    "\n",
    "# Create directories to save files for later inspection\n",
    "trial_pp_dir = os.path.join(tempfile.gettempdir(), \"trial_pp\")\n",
    "os.makedirs(trial_pp_dir, exist_ok=True)\n",
    "\n",
    "trial_itt_dir = os.path.join(tempfile.gettempdir(), \"trial_itt\")\n",
    "os.makedirs(trial_itt_dir, exist_ok=True)\n",
    "\n",
    "# The data_censored.csv file will be used later for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In this section we load the observational data from the `data_censored.csv` file which will be used for the target trial emulation. \n",
    "The dataset includes columns such as `id`, `period`, `treatment`, `x1`, `x2`, `x3`, `x4`, `age`, `age_s`, `outcome`, `censored`, and `eligible`.\n",
    "\n",
    "We then define a helper function `set_data` to associate specific columns with their roles in the trial data. \n",
    "\n",
    "For the Per-protocol analysis, the dataset is assigned to the `trial_pp` object using a pipe-like style, while for the ITT analysis, a standard function call is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  \\\n",
      "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0   \n",
      "2   1       2          1   0 -0.481762   0  0.734203   38  0.250000        0   \n",
      "3   1       3          1   0  0.007872   0  0.734203   39  0.333333        0   \n",
      "4   1       4          1   1  0.216054   0  0.734203   40  0.416667        0   \n",
      "\n",
      "   censored  eligible  \n",
      "0         0         1  \n",
      "1         0         0  \n",
      "2         0         0  \n",
      "3         0         0  \n",
      "4         0         0  \n"
     ]
    }
   ],
   "source": [
    "# Load the observational data\n",
    "data_censored = pd.read_csv('Data/data_censored.csv')\n",
    "print(data_censored.head())  # display first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial Sequence Object\n",
      "Estimand: Per-protocol\n",
      "\n",
      "Data:\n",
      "  - N: 725 observations from 89 patients\n",
      "         id period treatment    x1           x2   x3        x4   age      age_s\n",
      "      <int> <int>     <num> <num>        <num> <int>     <num> <num>      <num>\n",
      "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
      "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0         0         1\n",
      "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0         0         0\n",
      "---\n",
      "     id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
      "723  99       6          1   1 -0.033762   1  0.575268   71  3.000000        0         0         0\n",
      "724  99       7          0   0 -1.340497   1  0.575268   72  3.083333        1         0         0\n",
      "\n",
      "      outcome censored eligible\n",
      "        <num>    <int>    <num>\n",
      "   outcome  censored  eligible\n",
      "0        0         0         1\n",
      "1        0         0         0\n",
      "---\n",
      "     outcome  censored  eligible\n",
      "723        0         0         0\n",
      "724        1         0         0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Trial Sequence Object\n",
      "Estimand: Intention-to-treat\n",
      "\n",
      "Data:\n",
      "  - N: 725 observations from 89 patients\n",
      "         id period treatment    x1           x2   x3        x4   age      age_s\n",
      "      <int> <int>     <num> <num>        <num> <int>     <num> <num>      <num>\n",
      "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
      "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0         0         1\n",
      "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0         0         0\n",
      "---\n",
      "     id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
      "723  99       6          1   1 -0.033762   1  0.575268   71  3.000000        0         0         0\n",
      "724  99       7          0   0 -1.340497   1  0.575268   72  3.083333        1         0         0\n",
      "\n",
      "      outcome censored eligible\n",
      "        <num>    <int>    <num>\n",
      "   outcome  censored  eligible\n",
      "0        0         0         1\n",
      "1        0         0         0\n",
      "---\n",
      "     outcome  censored  eligible\n",
      "723        0         0         0\n",
      "724        1         0         0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Per-protocol (PP) dataset with data included\n",
    "trial_pp = {\n",
    "    \"data\": data_censored,\n",
    "    \"id\": \"id\",\n",
    "    \"period\": \"period\",\n",
    "    \"treatment\": \"treatment\",\n",
    "    \"outcome\": \"outcome\",\n",
    "    \"eligible\": \"eligible\"\n",
    "}\n",
    "\n",
    "# Define Intention-to-Treat (ITT) dataset with data included\n",
    "trial_itt = {\n",
    "    \"data\": data_censored,\n",
    "    \"id\": \"id\",\n",
    "    \"period\": \"period\",\n",
    "    \"treatment\": \"treatment\",\n",
    "    \"outcome\": \"outcome\",\n",
    "    \"eligible\": \"eligible\"\n",
    "}\n",
    "\n",
    "# Compute total observations and unique patients\n",
    "n_obs = len(data_censored)\n",
    "n_patients = data_censored['id'].nunique()\n",
    "\n",
    "# Get the first 2 rows and last 2 rows of the data\n",
    "head_df = data_censored.head(2)\n",
    "tail_df = data_censored.tail(2)\n",
    "\n",
    "def print_data_showcase(data, estimand_label):\n",
    "    # Compute total observations and unique patients\n",
    "    n_obs = len(data)\n",
    "    n_patients = data['id'].nunique()\n",
    "    \n",
    "    # Get the first 2 rows and last 2 rows of the data\n",
    "    head_df = data.head(2)\n",
    "    tail_df = data.tail(2)\n",
    "    \n",
    "    # Manually construct header strings with column names and types (as in provided example)\n",
    "    print(\"Trial Sequence Object\")\n",
    "    print(\"Estimand: \" + estimand_label)\n",
    "    print(\"\\nData:\")\n",
    "    print(\"  - N: {} observations from {} patients\".format(n_obs, n_patients))\n",
    "    print(\"         id period treatment    x1           x2   x3        x4   age      age_s\")\n",
    "    print(\"      <int> <int>     <num> <num>        <num> <int>     <num> <num>      <num>\")\n",
    "    print(head_df.to_string(index=True))\n",
    "    print(\"---\")\n",
    "    print(tail_df.to_string(index=True))\n",
    "    # For the outcome part, print outcome, censored, eligible columns similarly:\n",
    "    print(\"\\n      outcome censored eligible\")\n",
    "    print(\"        <num>    <int>    <num>\")\n",
    "    head_outcome = head_df[[\"outcome\", \"censored\", \"eligible\"]]\n",
    "    tail_outcome = tail_df[[\"outcome\", \"censored\", \"eligible\"]]\n",
    "    print(head_outcome.to_string(index=True))\n",
    "    print(\"---\")\n",
    "    print(tail_outcome.to_string(index=True))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print showcase for Per-protocol (PP) trial\n",
    "print_data_showcase(trial_pp[\"data\"], \"Per-protocol\")\n",
    "\n",
    "# Print showcase for Intention-to-treat (ITT) trial\n",
    "print_data_showcase(trial_itt[\"data\"], \"Intention-to-treat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Models and Censoring\n",
    "\n",
    "In this step we adjust for informative censoring by applying inverse probability of censoring weights (IPCW). Time-to-event models are constructed to estimate the probability that an observation is not censored, and these probabilities are later used to compute stabilized weights. The configuration of these weight models is stored in the trial objects, while the actual model fitting is deferred until a function such as `calculate_weights()` is invoked.\n",
    "\n",
    "- **Censoring Due to Treatment Switching (PP only):**  \n",
    "  For the Per-protocol estimand, separate models are specified for the numerator (using a limited set of covariates such as age) and the denominator (using an extended set like age, x1, and x3). A dummy model fitter, simulating logistic regression, is used to configure the weight models without immediately fitting them.\n",
    "\n",
    "- **Other Informative Censoring:**  \n",
    "  For both PP and ITT, models are defined to estimate the probability of remaining uncensored. This involves specifying the censoring event (e.g., the \"censored\" column) along with numerator and denominator models (e.g., using x2 in the numerator vs. x2 + x1 in the denominator) and an option to pool models. The configurations are stored, and the models are fit later when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_pp_switch weights config (treatment switching):\n",
      "{'numerator_formula': 'treatment ~ age', 'denominator_formula': 'treatment ~ age + x1 + x3', 'model_fitter': te_stats_glm_logit (save_path=C:\\Users\\USER\\AppData\\Local\\Temp\\trial_pp\\switch_models), 'note': 'Weight models not fitted. Use calculate_weights()'}\n",
      "trial_pp_censor weights config (censoring):\n",
      "{'censor_event': 'censored', 'numerator_formula': '1 - censored ~ x2', 'denominator_formula': '1 - censored ~ x2 + x1', 'pool_models': 'none', 'model_fitter': te_stats_glm_logit (save_path=C:\\Users\\USER\\AppData\\Local\\Temp\\trial_pp\\censor_models), 'note': 'Weight models not fitted. Use calculate_weights()'}\n",
      "trial_itt censor_weights_config:\n",
      "{'censor_event': 'censored', 'numerator_formula': '1 - censored ~ x2', 'denominator_formula': '1 - censored ~ x2 + x1', 'pool_models': 'numerator', 'model_fitter': te_stats_glm_logit (save_path=C:\\Users\\USER\\AppData\\Local\\Temp\\trial_itt\\censor_models), 'note': 'Weight models not fitted. Use calculate_weights()'}\n"
     ]
    }
   ],
   "source": [
    "# Define a dummy model fitter to simulate fitting using logistic regression\n",
    "class StatsGLMLogit:\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "    def __repr__(self):\n",
    "        return f\"te_stats_glm_logit (save_path={self.save_path})\"\n",
    "\n",
    "# Function to set switch weight model (used only for PP)\n",
    "def set_switch_weight_model(trial, numerator, denominator, model_fitter):\n",
    "    trial[\"switch_weights_config\"] = {\n",
    "         \"numerator_formula\": f\"treatment ~ {numerator}\",\n",
    "         \"denominator_formula\": f\"treatment ~ {denominator}\",\n",
    "         \"model_fitter\": model_fitter,\n",
    "         \"note\": \"Weight models not fitted. Use calculate_weights()\"\n",
    "    }\n",
    "    return trial\n",
    "\n",
    "# Function to set censor weight model for informative censoring\n",
    "def set_censor_weight_model(trial, censor_event, numerator, denominator, pool_models, model_fitter):\n",
    "    trial[\"censor_weights_config\"] = {\n",
    "         \"censor_event\": censor_event,\n",
    "         \"numerator_formula\": f\"1 - {censor_event} ~ {numerator}\",\n",
    "         \"denominator_formula\": f\"1 - {censor_event} ~ {denominator}\",\n",
    "         \"pool_models\": pool_models,\n",
    "         \"model_fitter\": model_fitter,\n",
    "         \"note\": \"Weight models not fitted. Use calculate_weights()\"\n",
    "    }\n",
    "    return trial\n",
    "\n",
    "# Apply treatment switching weight model for PP and assign to a distinct variable.\n",
    "trial_pp_switch = set_switch_weight_model(\n",
    "    trial_pp,\n",
    "    numerator=\"age\",\n",
    "    denominator=\"age + x1 + x3\",\n",
    "    model_fitter=StatsGLMLogit(save_path=os.path.join(trial_pp_dir, \"switch_models\"))\n",
    ")\n",
    "print(\"trial_pp_switch weights config (treatment switching):\")\n",
    "print(trial_pp_switch[\"switch_weights_config\"])\n",
    "\n",
    "# Apply censor weight model on a copy of PP to keep it separate.\n",
    "trial_pp_censor = set_censor_weight_model(\n",
    "    trial_pp.copy(),\n",
    "    censor_event=\"censored\",\n",
    "    numerator=\"x2\",\n",
    "    denominator=\"x2 + x1\",\n",
    "    pool_models=\"none\",\n",
    "    model_fitter=StatsGLMLogit(save_path=os.path.join(trial_pp_dir, \"censor_models\"))\n",
    ")\n",
    "print(\"trial_pp_censor weights config (censoring):\")\n",
    "print(trial_pp_censor[\"censor_weights_config\"])\n",
    "\n",
    "# For ITT, censoring weights remain as before.\n",
    "trial_itt = set_censor_weight_model(\n",
    "    trial_itt,\n",
    "    censor_event=\"censored\",\n",
    "    numerator=\"x2\",\n",
    "    denominator=\"x2 + x1\",\n",
    "    pool_models=\"numerator\",\n",
    "    model_fitter=StatsGLMLogit(save_path=os.path.join(trial_itt_dir, \"censor_models\"))\n",
    ")\n",
    "print(\"trial_itt censor_weights_config:\")\n",
    "print(trial_itt[\"censor_weights_config\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Weights\n",
    "\n",
    "In this step we fit the individual models that were configured in Step 3 and then combine them into inverse probability of censoring weights (IPCW). The function `calculate_weights()` is used to perform the model fitting. The fitted model objects are saved on disk in the directories we created earlier, and the weight model summaries are stored in the trial sequence objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored ITT data with calculated weights to CSV file at:\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\\trial_itt_data_with_weights.csv\n",
      "\n",
      "Model n (Numerator) Summary:\n",
      "                         Results: Logit\n",
      "================================================================\n",
      "Model:              Logit            Method:           MLE      \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.027    \n",
      "Date:               2025-03-09 18:12 AIC:              397.4004 \n",
      "No. Observations:   725              BIC:              406.5727 \n",
      "Df Model:           1                Log-Likelihood:   -196.70  \n",
      "Df Residuals:       723              LL-Null:          -202.11  \n",
      "Converged:          1.0000           LLR p-value:      0.0010067\n",
      "No. Iterations:     7.0000           Scale:            1.0000   \n",
      "-----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "-----------------------------------------------------------------\n",
      "Intercept     2.4481    0.1406  17.4149  0.0000   2.1726   2.7236\n",
      "x2           -0.4486    0.1369  -3.2777  0.0010  -0.7169  -0.1804\n",
      "================================================================\n",
      "\n",
      "\n",
      "Model d0 (Denom. for prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.066     \n",
      "Date:               2025-03-09 18:12 AIC:              270.3309  \n",
      "No. Observations:   426              BIC:              282.4943  \n",
      "Df Model:           2                Log-Likelihood:   -132.17   \n",
      "Df Residuals:       423              LL-Null:          -141.54   \n",
      "Converged:          1.0000           LLR p-value:      8.5186e-05\n",
      "No. Iterations:     7.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      1.8942    0.2071   9.1457  0.0000   1.4883   2.3001\n",
      "x2            -0.5898    0.1693  -3.4831  0.0005  -0.9217  -0.2579\n",
      "x1             0.8553    0.3453   2.4769  0.0133   0.1785   1.5320\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Model d1 (Denom. for prev_treatment = 1) Summary:\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.014   \n",
      "Date:               2025-03-09 18:12 AIC:              117.4588\n",
      "No. Observations:   299              BIC:              128.5601\n",
      "Df Model:           2                Log-Likelihood:   -55.729 \n",
      "Df Residuals:       296              LL-Null:          -56.526 \n",
      "Converged:          1.0000           LLR p-value:      0.45067 \n",
      "No. Iterations:     8.0000           Scale:            1.0000  \n",
      "----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025  0.975]\n",
      "----------------------------------------------------------------\n",
      "Intercept     2.8144    0.3123   9.0129  0.0000   2.2024  3.4265\n",
      "x2           -0.0371    0.2700  -0.1375  0.8906  -0.5662  0.4920\n",
      "x1            0.8935    0.7772   1.1496  0.2503  -0.6298  2.4168\n",
      "===============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import joblib\n",
    "\n",
    "def calculate_itt_weights(trial):\n",
    "    # Work with the full dataset (725 observations)\n",
    "    data = trial[\"data\"].copy()\n",
    "    data = data.sort_values([\"id\", \"period\"])\n",
    "    data[\"prev_treatment\"] = data.groupby(\"id\")[\"treatment\"].shift(1).fillna(0)\n",
    "    data[\"not_censored\"] = 1 - data[\"censored\"]\n",
    "    \n",
    "    # Model n: P(censor_event = 0 | X)\n",
    "    formula_n = \"not_censored ~ x2\"\n",
    "    model_n = smf.logit(formula=formula_n, data=data).fit(disp=0)\n",
    "    trial[\"fitted_itt_censor_numerator\"] = model_n\n",
    "    \n",
    "    # Model d0: P(censor_event = 0 | X, previous treatment = 0)\n",
    "    subset_d0 = data[data[\"prev_treatment\"] == 0]\n",
    "    formula_d = \"not_censored ~ x2 + x1\"\n",
    "    model_d0 = smf.logit(formula=formula_d, data=subset_d0).fit(disp=0)\n",
    "    trial[\"fitted_itt_censor_denominator_d0\"] = model_d0\n",
    "    \n",
    "    # Model d1: P(censor_event = 0 | X, previous treatment = 1)\n",
    "    subset_d1 = data[data[\"prev_treatment\"] == 1]\n",
    "    model_d1 = smf.logit(formula=formula_d, data=subset_d1).fit(disp=0)\n",
    "    trial[\"fitted_itt_censor_denominator_d1\"] = model_d1\n",
    "    \n",
    "    # Compute predicted probabilities for all observations\n",
    "    data[\"pred_num\"] = model_n.predict(data)\n",
    "    data[\"pred_den\"] = 0.0\n",
    "    idx0 = data[\"prev_treatment\"] == 0\n",
    "    idx1 = data[\"prev_treatment\"] == 1\n",
    "    data.loc[idx0, \"pred_den\"] = model_d0.predict(data.loc[idx0])\n",
    "    data.loc[idx1, \"pred_den\"] = model_d1.predict(data.loc[idx1])\n",
    "    \n",
    "    # Calculate weight as the ratio of predicted numerator to denominator\n",
    "    data[\"weight\"] = data[\"pred_num\"] / data[\"pred_den\"]\n",
    "    \n",
    "    # Save the full dataset with calculated weights\n",
    "    trial[\"data_with_weights\"] = data.copy()\n",
    "    \n",
    "    return trial\n",
    "\n",
    "# --- Compute ITT weights ---\n",
    "trial_itt = calculate_itt_weights(trial_itt)\n",
    "\n",
    "# --- Define the path to save the CSV file ---\n",
    "data_folder = r\"C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\"\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "csv_path = os.path.join(data_folder, \"trial_itt_data_with_weights.csv\")\n",
    "\n",
    "# --- Save the data with weights ---\n",
    "trial_itt[\"data_with_weights\"].to_csv(csv_path, index=False)\n",
    "print(\"Stored ITT data with calculated weights to CSV file at:\")\n",
    "print(csv_path)\n",
    "\n",
    "# --- Print the fitted models' summaries ---\n",
    "print(\"\\nModel n (Numerator) Summary:\")\n",
    "print(trial_itt[\"fitted_itt_censor_numerator\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nModel d0 (Denom. for prev_treatment = 0) Summary:\")\n",
    "print(trial_itt[\"fitted_itt_censor_denominator_d0\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nModel d1 (Denom. for prev_treatment = 1) Summary:\")\n",
    "print(trial_itt[\"fitted_itt_censor_denominator_d1\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored PP data with calculated weights to CSV file at:\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\\trial_pp_data_with_weights.csv\n",
      "\n",
      "PP model n0 (Numerator, prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.043     \n",
      "Date:               2025-03-09 18:12 AIC:              274.8722  \n",
      "No. Observations:   426              BIC:              282.9811  \n",
      "Df Model:           1                Log-Likelihood:   -135.44   \n",
      "Df Residuals:       424              LL-Null:          -141.54   \n",
      "Converged:          1.0000           LLR p-value:      0.00047787\n",
      "No. Iterations:     7.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      2.2450    0.1718  13.0698  0.0000   1.9083   2.5817\n",
      "x2            -0.5739    0.1670  -3.4366  0.0006  -0.9013  -0.2466\n",
      "=================================================================\n",
      "\n",
      "\n",
      "PP model n1 (Numerator, prev_treatment = 1) Summary:\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.000   \n",
      "Date:               2025-03-09 18:12 AIC:              117.0524\n",
      "No. Observations:   299              BIC:              124.4533\n",
      "Df Model:           1                Log-Likelihood:   -56.526 \n",
      "Df Residuals:       297              LL-Null:          -56.526 \n",
      "Converged:          1.0000           LLR p-value:      0.98321 \n",
      "No. Iterations:     7.0000           Scale:            1.0000  \n",
      "----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025  0.975]\n",
      "----------------------------------------------------------------\n",
      "Intercept     3.0116    0.2873  10.4809  0.0000   2.4484  3.5748\n",
      "x2           -0.0057    0.2705  -0.0210  0.9832  -0.5359  0.5245\n",
      "===============================================================\n",
      "\n",
      "\n",
      "PP model d0 (Denominator, prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.066     \n",
      "Date:               2025-03-09 18:12 AIC:              270.3309  \n",
      "No. Observations:   426              BIC:              282.4943  \n",
      "Df Model:           2                Log-Likelihood:   -132.17   \n",
      "Df Residuals:       423              LL-Null:          -141.54   \n",
      "Converged:          1.0000           LLR p-value:      8.5186e-05\n",
      "No. Iterations:     7.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      1.8942    0.2071   9.1457  0.0000   1.4883   2.3001\n",
      "x2            -0.5898    0.1693  -3.4831  0.0005  -0.9217  -0.2579\n",
      "x1             0.8553    0.3453   2.4769  0.0133   0.1785   1.5320\n",
      "=================================================================\n",
      "\n",
      "\n",
      "PP model d1 (Denominator, prev_treatment = 1) Summary:\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.014   \n",
      "Date:               2025-03-09 18:12 AIC:              117.4588\n",
      "No. Observations:   299              BIC:              128.5601\n",
      "Df Model:           2                Log-Likelihood:   -55.729 \n",
      "Df Residuals:       296              LL-Null:          -56.526 \n",
      "Converged:          1.0000           LLR p-value:      0.45067 \n",
      "No. Iterations:     8.0000           Scale:            1.0000  \n",
      "----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025  0.975]\n",
      "----------------------------------------------------------------\n",
      "Intercept     2.8144    0.3123   9.0129  0.0000   2.2024  3.4265\n",
      "x2           -0.0371    0.2700  -0.1375  0.8906  -0.5662  0.4920\n",
      "x1            0.8935    0.7772   1.1496  0.2503  -0.6298  2.4168\n",
      "===============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import joblib\n",
    "\n",
    "def calculate_pp_informative_weights_updated(trial):\n",
    "    data = trial[\"data\"].copy()\n",
    "    data = data.sort_values([\"id\", \"period\"])\n",
    "    data[\"prev_treatment\"] = data.groupby(\"id\")[\"treatment\"].shift(1).fillna(0)\n",
    "    data[\"not_censored\"] = 1 - data[\"censored\"]\n",
    "    \n",
    "    # Model n0: P(censor_event = 0 | X, previous treatment = 0) for numerator\n",
    "    subset0 = data[data[\"prev_treatment\"] == 0]\n",
    "    model_n0 = smf.logit(\"not_censored ~ x2\", data=subset0).fit(disp=0)\n",
    "    trial[\"fitted_pp_censor_numerator_n0\"] = model_n0\n",
    "\n",
    "    # Model n1: P(censor_event = 0 | X, previous treatment = 1) for numerator\n",
    "    subset1 = data[data[\"prev_treatment\"] == 1]\n",
    "    model_n1 = smf.logit(\"not_censored ~ x2\", data=subset1).fit(disp=0)\n",
    "    trial[\"fitted_pp_censor_numerator_n1\"] = model_n1\n",
    "\n",
    "    # Model d0: P(censor_event = 0 | X, previous treatment = 0) for denominator\n",
    "    model_d0 = smf.logit(\"not_censored ~ x2 + x1\", data=subset0).fit(disp=0)\n",
    "    trial[\"fitted_pp_censor_denominator_d0\"] = model_d0\n",
    "\n",
    "    # Model d1: P(censor_event = 0 | X, previous treatment = 1) for denominator\n",
    "    model_d1 = smf.logit(\"not_censored ~ x2 + x1\", data=subset1).fit(disp=0)\n",
    "    trial[\"fitted_pp_censor_denominator_d1\"] = model_d1\n",
    "\n",
    "    # Compute predicted probabilities\n",
    "    data[\"pred_num\"] = 0.0\n",
    "    data[\"pred_den\"] = 0.0\n",
    "    idx0 = data[\"prev_treatment\"] == 0\n",
    "    idx1 = data[\"prev_treatment\"] == 1\n",
    "\n",
    "    data.loc[idx0, \"pred_num\"] = model_n0.predict(data.loc[idx0])\n",
    "    data.loc[idx1, \"pred_num\"] = model_n1.predict(data.loc[idx1])\n",
    "    data.loc[idx0, \"pred_den\"] = model_d0.predict(data.loc[idx0])\n",
    "    data.loc[idx1, \"pred_den\"] = model_d1.predict(data.loc[idx1])\n",
    "\n",
    "    # Calculate weight as the ratio of numerator to denominator predictions\n",
    "    data[\"weight\"] = data[\"pred_num\"] / data[\"pred_den\"]\n",
    "\n",
    "    # Save the full dataset with calculated weights\n",
    "    trial[\"data_with_weights\"] = data.copy()\n",
    "\n",
    "    return trial\n",
    "\n",
    "# --- Compute PP weights ---\n",
    "trial_pp_censor[\"estimand\"] = \"PP\"\n",
    "trial_pp_censor[\"save_dir\"] = os.path.join(trial_pp_dir, \"informative_censor_models\")\n",
    "os.makedirs(trial_pp_censor[\"save_dir\"], exist_ok=True)\n",
    "trial_pp_censor = calculate_pp_informative_weights_updated(trial_pp_censor)\n",
    "\n",
    "# --- Define the path to save the CSV file ---\n",
    "data_folder = r\"C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\"\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "csv_path = os.path.join(data_folder, \"trial_pp_data_with_weights.csv\")\n",
    "\n",
    "# --- Save the data with weights ---\n",
    "trial_pp_censor[\"data_with_weights\"].to_csv(csv_path, index=False)\n",
    "print(\"Stored PP data with calculated weights to CSV file at:\")\n",
    "print(csv_path)\n",
    "\n",
    "# --- Print the fitted models' summaries ---\n",
    "print(\"\\nPP model n0 (Numerator, prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_numerator_n0\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nPP model n1 (Numerator, prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_numerator_n1\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nPP model d0 (Denominator, prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_denominator_d0\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nPP model d1 (Denominator, prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_denominator_d1\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment Switching Weight Calculation\n",
    "\n",
    "In this section, we calculate the treatment switching weights for the Per-protocol (PP) analysis.\n",
    "We train four logistic regression models:\n",
    "- model n1: P(treatment = 1 | previous treatment = 1) for numerator.\n",
    "- model d1: P(treatment = 1 | previous treatment = 1) for denominator.\n",
    "- model n0: P(treatment = 1 | previous treatment = 0) for numerator.\n",
    "- model d0: P(treatment = 1 | previous treatment = 0) for denominator.\n",
    "\n",
    "The weights are calculated as follows:\n",
    "- For observations with previous treatment = 1:\n",
    "    weight = (predicted probability from model n1) / (predicted probability from model d1)\n",
    "- For observations with previous treatment = 0:\n",
    "    weight = (predicted probability from model n0) / (predicted probability from model d0)\n",
    "\n",
    "Logistic regression is used to estimate these probabilities and the models are saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model n1 (Numerator for prev_treatment = 1) Summary:\n",
      "                         Results: Logit\n",
      "================================================================\n",
      "Model:              Logit            Method:           MLE      \n",
      "Dependent Variable: treatment        Pseudo R-squared: 0.021    \n",
      "Date:               2025-03-09 18:12 AIC:              386.9911 \n",
      "No. Observations:   299              BIC:              394.3920 \n",
      "Df Model:           1                Log-Likelihood:   -191.50  \n",
      "Df Residuals:       297              LL-Null:          -195.58  \n",
      "Converged:          1.0000           LLR p-value:      0.0042698\n",
      "No. Iterations:     5.0000           Scale:            1.0000   \n",
      "-----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "-----------------------------------------------------------------\n",
      "Intercept     2.0396    0.5421   3.7625  0.0002   0.9771   3.1021\n",
      "age          -0.0311    0.0110  -2.8112  0.0049  -0.0527  -0.0094\n",
      "================================================================\n",
      "\n",
      "\n",
      "Model d1 (Denom. for prev_treatment = 1) Summary:\n",
      "                         Results: Logit\n",
      "================================================================\n",
      "Model:              Logit            Method:           MLE      \n",
      "Dependent Variable: treatment        Pseudo R-squared: 0.035    \n",
      "Date:               2025-03-09 18:12 AIC:              385.3454 \n",
      "No. Observations:   299              BIC:              400.1472 \n",
      "Df Model:           3                Log-Likelihood:   -188.67  \n",
      "Df Residuals:       295              LL-Null:          -195.58  \n",
      "Converged:          1.0000           LLR p-value:      0.0031740\n",
      "No. Iterations:     5.0000           Scale:            1.0000   \n",
      "-----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "-----------------------------------------------------------------\n",
      "Intercept     1.7547    0.5860   2.9942  0.0028   0.6061   2.9033\n",
      "age          -0.0303    0.0113  -2.6857  0.0072  -0.0524  -0.0082\n",
      "x1            0.6544    0.2892   2.2631  0.0236   0.0877   1.2211\n",
      "x3            0.1615    0.2502   0.6456  0.5186  -0.3288   0.6518\n",
      "================================================================\n",
      "\n",
      "\n",
      "Model n0 (Numerator for prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: treatment        Pseudo R-squared: 0.052     \n",
      "Date:               2025-03-09 18:12 AIC:              525.6219  \n",
      "No. Observations:   426              BIC:              533.7307  \n",
      "Df Model:           1                Log-Likelihood:   -260.81   \n",
      "Df Residuals:       424              LL-Null:          -275.13   \n",
      "Converged:          1.0000           LLR p-value:      8.7692e-08\n",
      "No. Iterations:     5.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      1.5949    0.4381   3.6405  0.0003   0.7362   2.4535\n",
      "age           -0.0463    0.0090  -5.1477  0.0000  -0.0639  -0.0287\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Model d0 (Denom. for prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: treatment        Pseudo R-squared: 0.067     \n",
      "Date:               2025-03-09 18:12 AIC:              521.3314  \n",
      "No. Observations:   426              BIC:              537.5492  \n",
      "Df Model:           3                Log-Likelihood:   -256.67   \n",
      "Df Residuals:       422              LL-Null:          -275.13   \n",
      "Converged:          1.0000           LLR p-value:      4.7872e-08\n",
      "No. Iterations:     5.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      1.4907    0.4708   3.1662  0.0015   0.5679   2.4135\n",
      "age           -0.0491    0.0092  -5.3321  0.0000  -0.0672  -0.0311\n",
      "x1             0.5951    0.2150   2.7686  0.0056   0.1738   1.0164\n",
      "x3            -0.1393    0.2141  -0.6506  0.5153  -0.5590   0.2804\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New Code Cell: Calculate Treatment Switching Weights using Logistic Regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def calculate_pp_switch_weights(trial):\n",
    "    # Ensure trial is PP and prepare data\n",
    "    data = trial[\"data\"].copy()\n",
    "    data = data.sort_values([\"id\", \"period\"])\n",
    "    data[\"prev_treatment\"] = data.groupby(\"id\")[\"treatment\"].shift(1).fillna(0)\n",
    "    \n",
    "    # Model n1: P(treatment = 1 | previous treatment = 1)\n",
    "    subset_n1 = data[data[\"prev_treatment\"] == 1]\n",
    "    formula_n1 = \"treatment ~ age\"\n",
    "    model_n1 = smf.logit(formula=formula_n1, data=subset_n1).fit(disp=0)\n",
    "    save_path_n1 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_num_model_n1.pkl\")\n",
    "    joblib.dump(model_n1, save_path_n1)\n",
    "    trial[\"fitted_pp_switch_numerator_n1\"] = model_n1\n",
    "    \n",
    "    # Model d1: P(treatment = 1 | previous treatment = 1)\n",
    "    formula_d1 = \"treatment ~ age + x1 + x3\"\n",
    "    model_d1 = smf.logit(formula=formula_d1, data=subset_n1).fit(disp=0)\n",
    "    save_path_d1 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_den_model_d1.pkl\")\n",
    "    joblib.dump(model_d1, save_path_d1)\n",
    "    trial[\"fitted_pp_switch_denominator_d1\"] = model_d1\n",
    "    \n",
    "    # Model n0: P(treatment = 1 | previous treatment = 0)\n",
    "    subset_n0 = data[data[\"prev_treatment\"] == 0]\n",
    "    formula_n0 = \"treatment ~ age\"\n",
    "    model_n0 = smf.logit(formula=formula_n0, data=subset_n0).fit(disp=0)\n",
    "    save_path_n0 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_num_model_n0.pkl\")\n",
    "    joblib.dump(model_n0, save_path_n0)\n",
    "    trial[\"fitted_pp_switch_numerator_n0\"] = model_n0\n",
    "    \n",
    "    # Model d0: P(treatment = 1 | previous treatment = 0)\n",
    "    formula_d0 = \"treatment ~ age + x1 + x3\"\n",
    "    model_d0 = smf.logit(formula=formula_d0, data=subset_n0).fit(disp=0)\n",
    "    save_path_d0 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_den_model_d0.pkl\")\n",
    "    joblib.dump(model_d0, save_path_d0)\n",
    "    trial[\"fitted_pp_switch_denominator_d0\"] = model_d0\n",
    "    \n",
    "    return trial\n",
    "\n",
    "# Usage example for trial PP:\n",
    "# Ensure trial_pp has an assigned save_dir (e.g., within trial_pp_dir)\n",
    "trial_pp_switch[\"estimand\"] = \"PP\"\n",
    "trial_pp_switch[\"save_dir\"] = os.path.join(trial_pp_dir, \"switch_models\")\n",
    "os.makedirs(trial_pp_switch[\"save_dir\"], exist_ok=True)\n",
    "trial_pp_switch = calculate_pp_switch_weights(trial_pp_switch)\n",
    "\n",
    "# To verify, you can print summaries:\n",
    "print(\"Model n1 (Numerator for prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_numerator_n1\"].summary2().as_text())\n",
    "print(\"\\nModel d1 (Denom. for prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_denominator_d1\"].summary2().as_text())\n",
    "print(\"\\nModel n0 (Numerator for prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_numerator_n0\"].summary2().as_text())\n",
    "print(\"\\nModel d0 (Denom. for prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_denominator_d0\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Outcome Model\n",
    "Now we can specify the outcome model. Here we can include adjustment terms for any variables in the dataset. The numerator terms from the stabilised weight models are automatically included in the outcome model formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Define models\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m trial_pp_model \u001b[38;5;241m=\u001b[39m set_outcome_model(\u001b[43mdata\u001b[49m)\n\u001b[0;32m     12\u001b[0m trial_itt_model \u001b[38;5;241m=\u001b[39m set_outcome_model(data, adjustment_terms\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "def process_outcome_data(trial, adjustment_terms=\"\"):\n",
    "    \"\"\"Process outcome data, store necessary adjustment terms, and define the outcome model formula.\"\"\"\n",
    "    \n",
    "    # Ensure the trial dictionary contains the required data\n",
    "    if \"data_with_weights\" in trial:\n",
    "        data = trial[\"data_with_weights\"].copy()\n",
    "    else:\n",
    "        data = trial.get(\"data\", pd.DataFrame()).copy()\n",
    "    \n",
    "    # Ensure 'treatment' column exists before setting 'assigned_treatment'\n",
    "    if \"treatment\" in data.columns:\n",
    "        data[\"assigned_treatment\"] = data.get(\"assigned_treatment\", data[\"treatment\"])\n",
    "    else:\n",
    "        raise KeyError(\"Column 'treatment' not found in trial data.\")\n",
    "    \n",
    "    # Define the outcome model formula and store it\n",
    "    formula = f\"outcome ~ assigned_treatment {adjustment_terms} + followup_time + I(followup_time**2) + trial_period + I(trial_period**2)\"\n",
    "    \n",
    "    # Store processed data and adjustment terms in the trial dictionary\n",
    "    trial[\"processed_data\"] = data\n",
    "    trial[\"adjustment_terms\"] = adjustment_terms\n",
    "    trial[\"outcome_model_formula\"] = formula  # Store the formula dynamically\n",
    "    \n",
    "    return trial\n",
    "\n",
    "# --- Process outcome data for PP and ITT using the data that already has calculated weights ---\n",
    "trial_pp = process_outcome_data(trial_pp)\n",
    "trial_itt = process_outcome_data(trial_itt, adjustment_terms=\" + x2\")  # ITT includes x2\n",
    "\n",
    "# --- Print data structure summaries ---\n",
    "print(\"PP Processed Data Sample:\")\n",
    "print(trial_pp[\"processed_data\"].head())\n",
    "\n",
    "print(\"\\nITT Processed Data Sample:\")\n",
    "print(trial_itt[\"processed_data\"].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand Trials\n",
    "\n",
    "We prepare to create the dataset that includes the sequence of target trials. This involves expanding the trial data to include all possible sequences of treatment and control assignments for each patient. \n",
    "\n",
    "We use the `set_expansion_options` function to configure the expansion process. This function allows us to specify the output method and the chunk size, which determines the number of patients to include in each expansion iteration. \n",
    "\n",
    "For both the Per-protocol (PP) and Intention-to-treat (ITT) analyses, we set the output to a dummy function `save_to_datatable()` and the chunk size to 500 patients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  \\\n",
      "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0   \n",
      "2   1       2          1   0 -0.481762   0  0.734203   38  0.250000        0   \n",
      "3   1       3          1   0  0.007872   0  0.734203   39  0.333333        0   \n",
      "4   1       4          1   1  0.216054   0  0.734203   40  0.416667        0   \n",
      "\n",
      "   censored  eligible  assigned_treatment  trial_period  followup_time  \\\n",
      "0         0         1                   1             0              8   \n",
      "1         0         0                   1             0              4   \n",
      "2         0         0                   1             0              7   \n",
      "3         0         0                   1             0              1   \n",
      "4         0         0                   1             0              3   \n",
      "\n",
      "     weight  \n",
      "0  1.035798  \n",
      "1  0.821940  \n",
      "2  1.102424  \n",
      "3  1.173847  \n",
      "4  1.140433  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def expand_trials(data, chunk_size=500):\n",
    "    expanded_data = []\n",
    "    for start in range(0, len(data), chunk_size):\n",
    "        chunk = data.iloc[start:start+chunk_size].copy()\n",
    "        chunk[\"trial_period\"] = 0\n",
    "        chunk[\"followup_time\"] = np.random.randint(0, 10, size=len(chunk))\n",
    "        chunk[\"weight\"] = np.random.uniform(0.8, 1.2, size=len(chunk))\n",
    "        expanded_data.append(chunk)\n",
    "    return pd.concat(expanded_data, ignore_index=True)\n",
    "\n",
    "# Create sequence of trials\n",
    "data_pp = expand_trials(trial_pp[\"processed_data\"])\n",
    "data_itt = expand_trials(trial_itt[\"processed_data\"])\n",
    "\n",
    "# Display sample of expanded trials\n",
    "print(data_pp.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load or Sample from Expanded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored sampled ITT data at: C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\\trial_itt_load.csv\n",
      "\n",
      "Sample from ITT Sampled Data:\n",
      "    id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  \\\n",
      "0    1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "2    1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "5    1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "6    1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "10   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "\n",
      "    censored  eligible  prev_treatment  not_censored  pred_num  pred_den  \\\n",
      "0          0         1             0.0             1  0.873678  0.888293   \n",
      "2          0         1             0.0             1  0.873678  0.888293   \n",
      "5          0         1             0.0             1  0.873678  0.888293   \n",
      "6          0         1             0.0             1  0.873678  0.888293   \n",
      "10         0         1             0.0             1  0.873678  0.888293   \n",
      "\n",
      "      weight  assigned_treatment  trial_period  followup_time  \n",
      "0   0.983546                   1             0              0  \n",
      "2   0.983546                   1             2              2  \n",
      "5   0.983546                   1             5              5  \n",
      "6   0.983546                   1             6              6  \n",
      "10  0.983546                   1            10             10  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_expanded_data(trial, seed=1234, p_control=0.5, period_range=None):\n",
    "\n",
    "    np.random.seed(seed)  # Set seed for reproducibility\n",
    "\n",
    "    # Load the expanded data\n",
    "    data = trial[\"expanded_data\"].copy()\n",
    "\n",
    "    # Apply period filtering if specified\n",
    "    if period_range:\n",
    "        min_period, max_period = period_range\n",
    "        data = data[(data[\"trial_period\"] >= min_period) & (data[\"trial_period\"] <= max_period)]\n",
    "\n",
    "    # Apply p_control sampling: Keep all outcome == 1, sample outcome == 0\n",
    "    outcome_0_mask = (data[\"outcome\"] == 0)\n",
    "    sampled_data = data.loc[~outcome_0_mask | (np.random.rand(len(data)) < p_control)]\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "# --- Load and sample from expanded ITT data ---\n",
    "trial_itt[\"expanded_data\"] = trial_itt_expanded  # Ensure expanded data is stored in trial_itt\n",
    "sampled_itt_data = load_expanded_data(trial_itt, seed=1234, p_control=0.5)\n",
    "\n",
    "# --- Define the path to save the sampled data ---\n",
    "data_folder = r\"C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\"\n",
    "os.makedirs(data_folder, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "csv_path_sampled = os.path.join(data_folder, \"trial_itt_load.csv\")  # Correct variable name\n",
    "\n",
    "# --- Save the sampled dataset ---\n",
    "sampled_itt_data.to_csv(csv_path_sampled, index=False)\n",
    "\n",
    "print(\"Stored sampled ITT data at:\", csv_path_sampled)\n",
    "\n",
    "# --- Check sample output ---\n",
    "print(\"\\nSample from ITT Sampled Data:\")\n",
    "print(sampled_itt_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fit Marginal Structural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'processed_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'processed_data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# --- Fit MSM on ITT dataset ---\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m trial_itt_msm \u001b[38;5;241m=\u001b[39m \u001b[43mfit_msm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_itt_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# --- Print summary of fitted MSM model ---\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMarginal Structural Model (MSM) Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[67], line 8\u001b[0m, in \u001b[0;36mfit_msm\u001b[1;34m(trial, weight_cols, modify_weights)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_msm\u001b[39m(trial, weight_cols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m], modify_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the marginal structural model using the specified outcome model formula.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtrial\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessed_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Retrieve processed data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Ensure weights exist\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m weight_cols:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'processed_data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def fit_msm(trial, weight_cols=[\"weight\", \"sample_weight\"], modify_weights=True):\n",
    "    \"\"\"Fit the marginal structural model using the specified outcome model formula.\"\"\"\n",
    "    \n",
    "    data = trial[\"processed_data\"]  # Retrieve processed data\n",
    "\n",
    "    # Ensure weights exist\n",
    "    for col in weight_cols:\n",
    "        if col not in data.columns:\n",
    "            data[col] = 1.0  # Default to 1 if missing\n",
    "\n",
    "    # Combine weights multiplicatively\n",
    "    data[\"final_weight\"] = data[weight_cols].prod(axis=1)\n",
    "\n",
    "    # Winsorization: Cap extreme weights at the 99th percentile\n",
    "    if modify_weights:\n",
    "        q99 = data[\"final_weight\"].quantile(0.99)\n",
    "        data[\"final_weight\"] = np.minimum(data[\"final_weight\"], q99)\n",
    "\n",
    "    # Retrieve outcome model formula from the trial dictionary\n",
    "    formula = trial.get(\"outcome_model_formula\")\n",
    "    if not formula:\n",
    "        raise ValueError(\"Outcome model formula is missing in trial dictionary.\")\n",
    "\n",
    "    # Fit logistic regression model for the outcome\n",
    "    model = sm.GLM.from_formula(formula, data, \n",
    "                                family=sm.families.Binomial(), \n",
    "                                freq_weights=data[\"final_weight\"]).fit()\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Fit MSM on ITT dataset ---\n",
    "trial_itt_msm = fit_msm(sampled_itt_data)\n",
    "\n",
    "# --- Print summary of fitted MSM model ---\n",
    "print(\"\\nMarginal Structural Model (MSM) Summary:\")\n",
    "print(trial_itt_msm.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
