{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup \n",
    "\n",
    "In this section we define our target trial estimands for two scenarios:\n",
    "- **Per-protocol (PP):** Focused on patients adhering strictly to the treatment protocol.\n",
    "- **Intention-to-treat (ITT):** Analyses based on the treatment as assigned regardless of adherence.\n",
    "\n",
    "We also create directories using Pythonâ€™s `tempfile` module to store model outputs or intermediate files for later inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "# Define the estimands\n",
    "estimand_pp = \"PP\"  # Per-protocol\n",
    "estimand_itt = \"ITT\"  # Intention-to-treat\n",
    "\n",
    "# Create directories to save files for later inspection\n",
    "trial_pp_dir = os.path.join(tempfile.gettempdir(), \"trial_pp\")\n",
    "os.makedirs(trial_pp_dir, exist_ok=True)\n",
    "\n",
    "trial_itt_dir = os.path.join(tempfile.gettempdir(), \"trial_itt\")\n",
    "os.makedirs(trial_itt_dir, exist_ok=True)\n",
    "\n",
    "# The data_censored.csv file will be used later for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In this section we load the observational data from the `data_censored.csv` file which will be used for the target trial emulation. \n",
    "The dataset includes columns such as `id`, `period`, `treatment`, `x1`, `x2`, `x3`, `x4`, `age`, `age_s`, `outcome`, `censored`, and `eligible`.\n",
    "\n",
    "We then define a helper function `set_data` to associate specific columns with their roles in the trial data. \n",
    "\n",
    "For the Per-protocol analysis, the dataset is assigned to the `trial_pp` object using a pipe-like style, while for the ITT analysis, a standard function call is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  \\\n",
      "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0   \n",
      "2   1       2          1   0 -0.481762   0  0.734203   38  0.250000        0   \n",
      "3   1       3          1   0  0.007872   0  0.734203   39  0.333333        0   \n",
      "4   1       4          1   1  0.216054   0  0.734203   40  0.416667        0   \n",
      "\n",
      "   censored  eligible  \n",
      "0         0         1  \n",
      "1         0         0  \n",
      "2         0         0  \n",
      "3         0         0  \n",
      "4         0         0  \n"
     ]
    }
   ],
   "source": [
    "# Load the observational data\n",
    "data_censored = pd.read_csv('data_censored.csv')\n",
    "print(data_censored.head())  # display first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial Sequence Object\n",
      "Estimand: Per-protocol\n",
      "\n",
      "Data:\n",
      "  - N: 725 observations from 89 patients\n",
      "         id period treatment    x1           x2   x3        x4   age      age_s\n",
      "      <int> <int>     <num> <num>        <num> <int>     <num> <num>      <num>\n",
      "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
      "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0         0         1\n",
      "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0         0         0\n",
      "---\n",
      "     id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
      "723  99       6          1   1 -0.033762   1  0.575268   71  3.000000        0         0         0\n",
      "724  99       7          0   0 -1.340497   1  0.575268   72  3.083333        1         0         0\n",
      "\n",
      "      outcome censored eligible\n",
      "        <num>    <int>    <num>\n",
      "   outcome  censored  eligible\n",
      "0        0         0         1\n",
      "1        0         0         0\n",
      "---\n",
      "     outcome  censored  eligible\n",
      "723        0         0         0\n",
      "724        1         0         0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Trial Sequence Object\n",
      "Estimand: Intention-to-treat\n",
      "\n",
      "Data:\n",
      "  - N: 725 observations from 89 patients\n",
      "         id period treatment    x1           x2   x3        x4   age      age_s\n",
      "      <int> <int>     <num> <num>        <num> <int>     <num> <num>      <num>\n",
      "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
      "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0         0         1\n",
      "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0         0         0\n",
      "---\n",
      "     id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
      "723  99       6          1   1 -0.033762   1  0.575268   71  3.000000        0         0         0\n",
      "724  99       7          0   0 -1.340497   1  0.575268   72  3.083333        1         0         0\n",
      "\n",
      "      outcome censored eligible\n",
      "        <num>    <int>    <num>\n",
      "   outcome  censored  eligible\n",
      "0        0         0         1\n",
      "1        0         0         0\n",
      "---\n",
      "     outcome  censored  eligible\n",
      "723        0         0         0\n",
      "724        1         0         0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Per-protocol (PP) dataset with data included\n",
    "trial_pp = {\n",
    "    \"data\": data_censored,\n",
    "    \"id\": \"id\",\n",
    "    \"period\": \"period\",\n",
    "    \"treatment\": \"treatment\",\n",
    "    \"outcome\": \"outcome\",\n",
    "    \"eligible\": \"eligible\"\n",
    "}\n",
    "\n",
    "# Define Intention-to-Treat (ITT) dataset with data included\n",
    "trial_itt = {\n",
    "    \"data\": data_censored,\n",
    "    \"id\": \"id\",\n",
    "    \"period\": \"period\",\n",
    "    \"treatment\": \"treatment\",\n",
    "    \"outcome\": \"outcome\",\n",
    "    \"eligible\": \"eligible\"\n",
    "}\n",
    "\n",
    "# Compute total observations and unique patients\n",
    "n_obs = len(data_censored)\n",
    "n_patients = data_censored['id'].nunique()\n",
    "\n",
    "# Get the first 2 rows and last 2 rows of the data\n",
    "head_df = data_censored.head(2)\n",
    "tail_df = data_censored.tail(2)\n",
    "\n",
    "def print_data_showcase(data, estimand_label):\n",
    "    # Compute total observations and unique patients\n",
    "    n_obs = len(data)\n",
    "    n_patients = data['id'].nunique()\n",
    "    \n",
    "    # Get the first 2 rows and last 2 rows of the data\n",
    "    head_df = data.head(2)\n",
    "    tail_df = data.tail(2)\n",
    "    \n",
    "    # Manually construct header strings with column names and types (as in provided example)\n",
    "    print(\"Trial Sequence Object\")\n",
    "    print(\"Estimand: \" + estimand_label)\n",
    "    print(\"\\nData:\")\n",
    "    print(\"  - N: {} observations from {} patients\".format(n_obs, n_patients))\n",
    "    print(\"         id period treatment    x1           x2   x3        x4   age      age_s\")\n",
    "    print(\"      <int> <int>     <num> <num>        <num> <int>     <num> <num>      <num>\")\n",
    "    print(head_df.to_string(index=True))\n",
    "    print(\"---\")\n",
    "    print(tail_df.to_string(index=True))\n",
    "    # For the outcome part, print outcome, censored, eligible columns similarly:\n",
    "    print(\"\\n      outcome censored eligible\")\n",
    "    print(\"        <num>    <int>    <num>\")\n",
    "    head_outcome = head_df[[\"outcome\", \"censored\", \"eligible\"]]\n",
    "    tail_outcome = tail_df[[\"outcome\", \"censored\", \"eligible\"]]\n",
    "    print(head_outcome.to_string(index=True))\n",
    "    print(\"---\")\n",
    "    print(tail_outcome.to_string(index=True))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print showcase for Per-protocol (PP) trial\n",
    "print_data_showcase(trial_pp[\"data\"], \"Per-protocol\")\n",
    "\n",
    "# Print showcase for Intention-to-treat (ITT) trial\n",
    "print_data_showcase(trial_itt[\"data\"], \"Intention-to-treat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Models and Censoring\n",
    "\n",
    "In this step we adjust for informative censoring by applying inverse probability of censoring weights (IPCW). Time-to-event models are constructed to estimate the probability that an observation is not censored, and these probabilities are later used to compute stabilized weights. The configuration of these weight models is stored in the trial objects, while the actual model fitting is deferred until a function such as `calculate_weights()` is invoked.\n",
    "\n",
    "- **Censoring Due to Treatment Switching (PP only):**  \n",
    "  For the Per-protocol estimand, separate models are specified for the numerator (using a limited set of covariates such as age) and the denominator (using an extended set like age, x1, and x3). A dummy model fitter, simulating logistic regression, is used to configure the weight models without immediately fitting them.\n",
    "\n",
    "- **Other Informative Censoring:**  \n",
    "  For both PP and ITT, models are defined to estimate the probability of remaining uncensored. This involves specifying the censoring event (e.g., the \"censored\" column) along with numerator and denominator models (e.g., using x2 in the numerator vs. x2 + x1 in the denominator) and an option to pool models. The configurations are stored, and the models are fit later when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_pp_switch weights config (treatment switching):\n",
      "{'numerator_formula': 'treatment ~ age', 'denominator_formula': 'treatment ~ age + x1 + x3', 'model_fitter': te_stats_glm_logit (save_path=C:\\Users\\USER\\AppData\\Local\\Temp\\trial_pp\\switch_models), 'note': 'Weight models not fitted. Use calculate_weights()'}\n",
      "trial_pp_censor weights config (censoring):\n",
      "{'censor_event': 'censored', 'numerator_formula': '1 - censored ~ x2', 'denominator_formula': '1 - censored ~ x2 + x1', 'pool_models': 'none', 'model_fitter': te_stats_glm_logit (save_path=C:\\Users\\USER\\AppData\\Local\\Temp\\trial_pp\\censor_models), 'note': 'Weight models not fitted. Use calculate_weights()'}\n",
      "trial_itt censor_weights_config:\n",
      "{'censor_event': 'censored', 'numerator_formula': '1 - censored ~ x2', 'denominator_formula': '1 - censored ~ x2 + x1', 'pool_models': 'numerator', 'model_fitter': te_stats_glm_logit (save_path=C:\\Users\\USER\\AppData\\Local\\Temp\\trial_itt\\censor_models), 'note': 'Weight models not fitted. Use calculate_weights()'}\n"
     ]
    }
   ],
   "source": [
    "# Define a dummy model fitter to simulate fitting using logistic regression\n",
    "class StatsGLMLogit:\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "    def __repr__(self):\n",
    "        return f\"te_stats_glm_logit (save_path={self.save_path})\"\n",
    "\n",
    "# Function to set switch weight model (used only for PP)\n",
    "def set_switch_weight_model(trial, numerator, denominator, model_fitter):\n",
    "    trial[\"switch_weights_config\"] = {\n",
    "         \"numerator_formula\": f\"treatment ~ {numerator}\",\n",
    "         \"denominator_formula\": f\"treatment ~ {denominator}\",\n",
    "         \"model_fitter\": model_fitter,\n",
    "         \"note\": \"Weight models not fitted. Use calculate_weights()\"\n",
    "    }\n",
    "    return trial\n",
    "\n",
    "# Function to set censor weight model for informative censoring\n",
    "def set_censor_weight_model(trial, censor_event, numerator, denominator, pool_models, model_fitter):\n",
    "    trial[\"censor_weights_config\"] = {\n",
    "         \"censor_event\": censor_event,\n",
    "         \"numerator_formula\": f\"1 - {censor_event} ~ {numerator}\",\n",
    "         \"denominator_formula\": f\"1 - {censor_event} ~ {denominator}\",\n",
    "         \"pool_models\": pool_models,\n",
    "         \"model_fitter\": model_fitter,\n",
    "         \"note\": \"Weight models not fitted. Use calculate_weights()\"\n",
    "    }\n",
    "    return trial\n",
    "\n",
    "# Apply treatment switching weight model for PP and assign to a distinct variable.\n",
    "trial_pp_switch = set_switch_weight_model(\n",
    "    trial_pp,\n",
    "    numerator=\"age\",\n",
    "    denominator=\"age + x1 + x3\",\n",
    "    model_fitter=StatsGLMLogit(save_path=os.path.join(trial_pp_dir, \"switch_models\"))\n",
    ")\n",
    "print(\"trial_pp_switch weights config (treatment switching):\")\n",
    "print(trial_pp_switch[\"switch_weights_config\"])\n",
    "\n",
    "# Apply censor weight model on a copy of PP to keep it separate.\n",
    "trial_pp_censor = set_censor_weight_model(\n",
    "    trial_pp.copy(),\n",
    "    censor_event=\"censored\",\n",
    "    numerator=\"x2\",\n",
    "    denominator=\"x2 + x1\",\n",
    "    pool_models=\"none\",\n",
    "    model_fitter=StatsGLMLogit(save_path=os.path.join(trial_pp_dir, \"censor_models\"))\n",
    ")\n",
    "print(\"trial_pp_censor weights config (censoring):\")\n",
    "print(trial_pp_censor[\"censor_weights_config\"])\n",
    "\n",
    "# For ITT, censoring weights remain as before.\n",
    "trial_itt = set_censor_weight_model(\n",
    "    trial_itt,\n",
    "    censor_event=\"censored\",\n",
    "    numerator=\"x2\",\n",
    "    denominator=\"x2 + x1\",\n",
    "    pool_models=\"numerator\",\n",
    "    model_fitter=StatsGLMLogit(save_path=os.path.join(trial_itt_dir, \"censor_models\"))\n",
    ")\n",
    "print(\"trial_itt censor_weights_config:\")\n",
    "print(trial_itt[\"censor_weights_config\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Weights\n",
    "\n",
    "In this step we fit the individual models that were configured in Step 3 and then combine them into inverse probability of censoring weights (IPCW). The function `calculate_weights()` is used to perform the model fitting. The fitted model objects are saved on disk in the directories we created earlier, and the weight model summaries are stored in the trial sequence objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model n (Numerator) Summary:\n",
      "                         Results: Logit\n",
      "================================================================\n",
      "Model:              Logit            Method:           MLE      \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.027    \n",
      "Date:               2025-03-09 10:44 AIC:              397.4004 \n",
      "No. Observations:   725              BIC:              406.5727 \n",
      "Df Model:           1                Log-Likelihood:   -196.70  \n",
      "Df Residuals:       723              LL-Null:          -202.11  \n",
      "Converged:          1.0000           LLR p-value:      0.0010067\n",
      "No. Iterations:     7.0000           Scale:            1.0000   \n",
      "-----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "-----------------------------------------------------------------\n",
      "Intercept     2.4481    0.1406  17.4149  0.0000   2.1726   2.7236\n",
      "x2           -0.4486    0.1369  -3.2777  0.0010  -0.7169  -0.1804\n",
      "================================================================\n",
      "\n",
      "\n",
      "Model d0 (Denom. for prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.066     \n",
      "Date:               2025-03-09 10:44 AIC:              270.3309  \n",
      "No. Observations:   426              BIC:              282.4943  \n",
      "Df Model:           2                Log-Likelihood:   -132.17   \n",
      "Df Residuals:       423              LL-Null:          -141.54   \n",
      "Converged:          1.0000           LLR p-value:      8.5186e-05\n",
      "No. Iterations:     7.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      1.8942    0.2071   9.1457  0.0000   1.4883   2.3001\n",
      "x2            -0.5898    0.1693  -3.4831  0.0005  -0.9217  -0.2579\n",
      "x1             0.8553    0.3453   2.4769  0.0133   0.1785   1.5320\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Model d1 (Denom. for prev_treatment = 1) Summary:\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.014   \n",
      "Date:               2025-03-09 10:44 AIC:              117.4588\n",
      "No. Observations:   299              BIC:              128.5601\n",
      "Df Model:           2                Log-Likelihood:   -55.729 \n",
      "Df Residuals:       296              LL-Null:          -56.526 \n",
      "Converged:          1.0000           LLR p-value:      0.45067 \n",
      "No. Iterations:     8.0000           Scale:            1.0000  \n",
      "----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025  0.975]\n",
      "----------------------------------------------------------------\n",
      "Intercept     2.8144    0.3123   9.0129  0.0000   2.2024  3.4265\n",
      "x2           -0.0371    0.2700  -0.1375  0.8906  -0.5662  0.4920\n",
      "x1            0.8935    0.7772   1.1496  0.2503  -0.6298  2.4168\n",
      "===============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New cell: Calculate ITT Informative Censoring Weights using Logistic Regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def calculate_itt_weights(trial):\n",
    "    # Ensure trial is ITT and prepare data\n",
    "    data = trial[\"data\"].copy()\n",
    "    data = data.sort_values([\"id\", \"period\"])\n",
    "    data[\"prev_treatment\"] = data.groupby(\"id\")[\"treatment\"].shift(1).fillna(0)\n",
    "    data[\"not_censored\"] = 1 - data[\"censored\"]\n",
    "    \n",
    "    # Model n: P(censor_event = 0 | X)\n",
    "    # Here, X is assumed to be x2 (can be adjusted as needed)\n",
    "    formula_n = \"not_censored ~ x2\"\n",
    "    model_n = smf.logit(formula=formula_n, data=data).fit(disp=0)\n",
    "    # Save numerator model to disk\n",
    "    save_path_n = os.path.join(trial.get(\"save_dir\", \"\"), \"itt_censor_num_model.pkl\")\n",
    "    joblib.dump(model_n, save_path_n)\n",
    "    trial[\"fitted_itt_censor_numerator\"] = model_n\n",
    "    \n",
    "    # Model d0: P(censor_event = 0 | X, previous treatment = 0)\n",
    "    subset_d0 = data[data[\"prev_treatment\"] == 0]\n",
    "    formula_d = \"not_censored ~ x2 + x1\"\n",
    "    model_d0 = smf.logit(formula=formula_d, data=subset_d0).fit(disp=0)\n",
    "    save_path_d0 = os.path.join(trial.get(\"save_dir\", \"\"), \"itt_censor_den_model_d0.pkl\")\n",
    "    joblib.dump(model_d0, save_path_d0)\n",
    "    trial[\"fitted_itt_censor_denominator_d0\"] = model_d0\n",
    "    \n",
    "    # Model d1: P(censor_event = 0 | X, previous treatment = 1)\n",
    "    subset_d1 = data[data[\"prev_treatment\"] == 1]\n",
    "    model_d1 = smf.logit(formula=formula_d, data=subset_d1).fit(disp=0)\n",
    "    save_path_d1 = os.path.join(trial.get(\"save_dir\", \"\"), \"itt_censor_den_model_d1.pkl\")\n",
    "    joblib.dump(model_d1, save_path_d1)\n",
    "    trial[\"fitted_itt_censor_denominator_d1\"] = model_d1\n",
    "    \n",
    "    return trial\n",
    "\n",
    "# Usage example for trial ITT:\n",
    "# Ensure trial_itt has an assigned save_dir (e.g., within trial_itt_dir)\n",
    "trial_itt[\"estimand\"] = \"ITT\"\n",
    "trial_itt[\"save_dir\"] = os.path.join(trial_itt_dir, \"censor_models\")\n",
    "os.makedirs(trial_itt[\"save_dir\"], exist_ok=True)\n",
    "trial_itt = calculate_itt_weights(trial_itt)\n",
    "\n",
    "# To verify, you can print summaries:\n",
    "print(\"Model n (Numerator) Summary:\")\n",
    "print(trial_itt[\"fitted_itt_censor_numerator\"].summary2().as_text())\n",
    "print(\"\\nModel d0 (Denom. for prev_treatment = 0) Summary:\")\n",
    "print(trial_itt[\"fitted_itt_censor_denominator_d0\"].summary2().as_text())\n",
    "print(\"\\nModel d1 (Denom. for prev_treatment = 1) Summary:\")\n",
    "print(trial_itt[\"fitted_itt_censor_denominator_d1\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP model n0 (Numerator, prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.043     \n",
      "Date:               2025-03-09 10:44 AIC:              274.8722  \n",
      "No. Observations:   426              BIC:              282.9811  \n",
      "Df Model:           1                Log-Likelihood:   -135.44   \n",
      "Df Residuals:       424              LL-Null:          -141.54   \n",
      "Converged:          1.0000           LLR p-value:      0.00047787\n",
      "No. Iterations:     7.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      2.2450    0.1718  13.0698  0.0000   1.9083   2.5817\n",
      "x2            -0.5739    0.1670  -3.4366  0.0006  -0.9013  -0.2466\n",
      "=================================================================\n",
      "\n",
      "\n",
      "PP model n1 (Numerator, prev_treatment = 1) Summary:\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.000   \n",
      "Date:               2025-03-09 10:44 AIC:              117.0524\n",
      "No. Observations:   299              BIC:              124.4533\n",
      "Df Model:           1                Log-Likelihood:   -56.526 \n",
      "Df Residuals:       297              LL-Null:          -56.526 \n",
      "Converged:          1.0000           LLR p-value:      0.98321 \n",
      "No. Iterations:     7.0000           Scale:            1.0000  \n",
      "----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025  0.975]\n",
      "----------------------------------------------------------------\n",
      "Intercept     3.0116    0.2873  10.4809  0.0000   2.4484  3.5748\n",
      "x2           -0.0057    0.2705  -0.0210  0.9832  -0.5359  0.5245\n",
      "===============================================================\n",
      "\n",
      "\n",
      "PP model d0 (Denominator, prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.066     \n",
      "Date:               2025-03-09 10:44 AIC:              270.3309  \n",
      "No. Observations:   426              BIC:              282.4943  \n",
      "Df Model:           2                Log-Likelihood:   -132.17   \n",
      "Df Residuals:       423              LL-Null:          -141.54   \n",
      "Converged:          1.0000           LLR p-value:      8.5186e-05\n",
      "No. Iterations:     7.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      1.8942    0.2071   9.1457  0.0000   1.4883   2.3001\n",
      "x2            -0.5898    0.1693  -3.4831  0.0005  -0.9217  -0.2579\n",
      "x1             0.8553    0.3453   2.4769  0.0133   0.1785   1.5320\n",
      "=================================================================\n",
      "\n",
      "\n",
      "PP model d1 (Denominator, prev_treatment = 1) Summary:\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.014   \n",
      "Date:               2025-03-09 10:44 AIC:              117.4588\n",
      "No. Observations:   299              BIC:              128.5601\n",
      "Df Model:           2                Log-Likelihood:   -55.729 \n",
      "Df Residuals:       296              LL-Null:          -56.526 \n",
      "Converged:          1.0000           LLR p-value:      0.45067 \n",
      "No. Iterations:     8.0000           Scale:            1.0000  \n",
      "----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025  0.975]\n",
      "----------------------------------------------------------------\n",
      "Intercept     2.8144    0.3123   9.0129  0.0000   2.2024  3.4265\n",
      "x2           -0.0371    0.2700  -0.1375  0.8906  -0.5662  0.4920\n",
      "x1            0.8935    0.7772   1.1496  0.2503  -0.6298  2.4168\n",
      "===============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New cell: Calculate PP Informative Censoring Weights (4 models) using trial PP data\n",
    "import statsmodels.formula.api as smf\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def calculate_pp_informative_weights_updated(trial):\n",
    "    data = trial[\"data\"].copy()\n",
    "    data = data.sort_values([\"id\", \"period\"])\n",
    "    data[\"prev_treatment\"] = data.groupby(\"id\")[\"treatment\"].shift(1).fillna(0)\n",
    "    data[\"not_censored\"] = 1 - data[\"censored\"]\n",
    "    \n",
    "    # Model n0: P(censor_event = 0 | X, previous treatment = 0) for numerator\n",
    "    subset0 = data[data[\"prev_treatment\"] == 0]\n",
    "    model_n0 = smf.logit(\"not_censored ~ x2\", data=subset0).fit(disp=0)\n",
    "    save_path_n0 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_censor_num_model_n0.pkl\")\n",
    "    joblib.dump(model_n0, save_path_n0)\n",
    "    trial[\"fitted_pp_censor_numerator_n0\"] = model_n0\n",
    "\n",
    "    # Model n1: P(censor_event = 0 | X, previous treatment = 1) for numerator\n",
    "    subset1 = data[data[\"prev_treatment\"] == 1]\n",
    "    model_n1 = smf.logit(\"not_censored ~ x2\", data=subset1).fit(disp=0)\n",
    "    save_path_n1 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_censor_num_model_n1.pkl\")\n",
    "    joblib.dump(model_n1, save_path_n1)\n",
    "    trial[\"fitted_pp_censor_numerator_n1\"] = model_n1\n",
    "\n",
    "    # Model d0: P(censor_event = 0 | X, previous treatment = 0) for denominator\n",
    "    model_d0 = smf.logit(\"not_censored ~ x2 + x1\", data=subset0).fit(disp=0)\n",
    "    save_path_d0 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_censor_den_model_d0.pkl\")\n",
    "    joblib.dump(model_d0, save_path_d0)\n",
    "    trial[\"fitted_pp_censor_denominator_d0\"] = model_d0\n",
    "\n",
    "    # Model d1: P(censor_event = 0 | X, previous treatment = 1) for denominator\n",
    "    model_d1 = smf.logit(\"not_censored ~ x2 + x1\", data=subset1).fit(disp=0)\n",
    "    save_path_d1 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_censor_den_model_d1.pkl\")\n",
    "    joblib.dump(model_d1, save_path_d1)\n",
    "    trial[\"fitted_pp_censor_denominator_d1\"] = model_d1\n",
    "\n",
    "    return trial\n",
    "\n",
    "# Usage example for trial PP:\n",
    "trial_pp_censor[\"estimand\"] = \"PP\"\n",
    "trial_pp_censor[\"save_dir\"] = os.path.join(trial_pp_dir, \"informative_censor_models\")\n",
    "os.makedirs(trial_pp_censor[\"save_dir\"], exist_ok=True)\n",
    "trial_pp_censor = calculate_pp_informative_weights_updated(trial_pp_censor)\n",
    "\n",
    "# Print model summaries to verify the fitting:\n",
    "print(\"PP model n0 (Numerator, prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_numerator_n0\"].summary2().as_text())\n",
    "print(\"\\nPP model n1 (Numerator, prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_numerator_n1\"].summary2().as_text())\n",
    "print(\"\\nPP model d0 (Denominator, prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_denominator_d0\"].summary2().as_text())\n",
    "print(\"\\nPP model d1 (Denominator, prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_denominator_d1\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment Switching Weight Calculation\n",
    "\n",
    "In this section, we calculate the treatment switching weights for the Per-protocol (PP) analysis.\n",
    "We train four logistic regression models:\n",
    "- model n1: P(treatment = 1 | previous treatment = 1) for numerator.\n",
    "- model d1: P(treatment = 1 | previous treatment = 1) for denominator.\n",
    "- model n0: P(treatment = 1 | previous treatment = 0) for numerator.\n",
    "- model d0: P(treatment = 1 | previous treatment = 0) for denominator.\n",
    "\n",
    "The weights are calculated as follows:\n",
    "- For observations with previous treatment = 1:\n",
    "    weight = (predicted probability from model n1) / (predicted probability from model d1)\n",
    "- For observations with previous treatment = 0:\n",
    "    weight = (predicted probability from model n0) / (predicted probability from model d0)\n",
    "\n",
    "Logistic regression is used to estimate these probabilities and the models are saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model n1 (Numerator for prev_treatment = 1) Summary:\n",
      "                         Results: Logit\n",
      "================================================================\n",
      "Model:              Logit            Method:           MLE      \n",
      "Dependent Variable: treatment        Pseudo R-squared: 0.021    \n",
      "Date:               2025-03-09 10:44 AIC:              386.9911 \n",
      "No. Observations:   299              BIC:              394.3920 \n",
      "Df Model:           1                Log-Likelihood:   -191.50  \n",
      "Df Residuals:       297              LL-Null:          -195.58  \n",
      "Converged:          1.0000           LLR p-value:      0.0042698\n",
      "No. Iterations:     5.0000           Scale:            1.0000   \n",
      "-----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "-----------------------------------------------------------------\n",
      "Intercept     2.0396    0.5421   3.7625  0.0002   0.9771   3.1021\n",
      "age          -0.0311    0.0110  -2.8112  0.0049  -0.0527  -0.0094\n",
      "================================================================\n",
      "\n",
      "\n",
      "Model d1 (Denom. for prev_treatment = 1) Summary:\n",
      "                         Results: Logit\n",
      "================================================================\n",
      "Model:              Logit            Method:           MLE      \n",
      "Dependent Variable: treatment        Pseudo R-squared: 0.035    \n",
      "Date:               2025-03-09 10:44 AIC:              385.3454 \n",
      "No. Observations:   299              BIC:              400.1472 \n",
      "Df Model:           3                Log-Likelihood:   -188.67  \n",
      "Df Residuals:       295              LL-Null:          -195.58  \n",
      "Converged:          1.0000           LLR p-value:      0.0031740\n",
      "No. Iterations:     5.0000           Scale:            1.0000   \n",
      "-----------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "-----------------------------------------------------------------\n",
      "Intercept     1.7547    0.5860   2.9942  0.0028   0.6061   2.9033\n",
      "age          -0.0303    0.0113  -2.6857  0.0072  -0.0524  -0.0082\n",
      "x1            0.6544    0.2892   2.2631  0.0236   0.0877   1.2211\n",
      "x3            0.1615    0.2502   0.6456  0.5186  -0.3288   0.6518\n",
      "================================================================\n",
      "\n",
      "\n",
      "Model n0 (Numerator for prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: treatment        Pseudo R-squared: 0.052     \n",
      "Date:               2025-03-09 10:44 AIC:              525.6219  \n",
      "No. Observations:   426              BIC:              533.7307  \n",
      "Df Model:           1                Log-Likelihood:   -260.81   \n",
      "Df Residuals:       424              LL-Null:          -275.13   \n",
      "Converged:          1.0000           LLR p-value:      8.7692e-08\n",
      "No. Iterations:     5.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      1.5949    0.4381   3.6405  0.0003   0.7362   2.4535\n",
      "age           -0.0463    0.0090  -5.1477  0.0000  -0.0639  -0.0287\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Model d0 (Denom. for prev_treatment = 0) Summary:\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Method:           MLE       \n",
      "Dependent Variable: treatment        Pseudo R-squared: 0.067     \n",
      "Date:               2025-03-09 10:44 AIC:              521.3314  \n",
      "No. Observations:   426              BIC:              537.5492  \n",
      "Df Model:           3                Log-Likelihood:   -256.67   \n",
      "Df Residuals:       422              LL-Null:          -275.13   \n",
      "Converged:          1.0000           LLR p-value:      4.7872e-08\n",
      "No. Iterations:     5.0000           Scale:            1.0000    \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Intercept      1.4907    0.4708   3.1662  0.0015   0.5679   2.4135\n",
      "age           -0.0491    0.0092  -5.3321  0.0000  -0.0672  -0.0311\n",
      "x1             0.5951    0.2150   2.7686  0.0056   0.1738   1.0164\n",
      "x3            -0.1393    0.2141  -0.6506  0.5153  -0.5590   0.2804\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New Code Cell: Calculate Treatment Switching Weights using Logistic Regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def calculate_pp_switch_weights(trial):\n",
    "    # Ensure trial is PP and prepare data\n",
    "    data = trial[\"data\"].copy()\n",
    "    data = data.sort_values([\"id\", \"period\"])\n",
    "    data[\"prev_treatment\"] = data.groupby(\"id\")[\"treatment\"].shift(1).fillna(0)\n",
    "    \n",
    "    # Model n1: P(treatment = 1 | previous treatment = 1)\n",
    "    subset_n1 = data[data[\"prev_treatment\"] == 1]\n",
    "    formula_n1 = \"treatment ~ age\"\n",
    "    model_n1 = smf.logit(formula=formula_n1, data=subset_n1).fit(disp=0)\n",
    "    save_path_n1 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_num_model_n1.pkl\")\n",
    "    joblib.dump(model_n1, save_path_n1)\n",
    "    trial[\"fitted_pp_switch_numerator_n1\"] = model_n1\n",
    "    \n",
    "    # Model d1: P(treatment = 1 | previous treatment = 1)\n",
    "    formula_d1 = \"treatment ~ age + x1 + x3\"\n",
    "    model_d1 = smf.logit(formula=formula_d1, data=subset_n1).fit(disp=0)\n",
    "    save_path_d1 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_den_model_d1.pkl\")\n",
    "    joblib.dump(model_d1, save_path_d1)\n",
    "    trial[\"fitted_pp_switch_denominator_d1\"] = model_d1\n",
    "    \n",
    "    # Model n0: P(treatment = 1 | previous treatment = 0)\n",
    "    subset_n0 = data[data[\"prev_treatment\"] == 0]\n",
    "    formula_n0 = \"treatment ~ age\"\n",
    "    model_n0 = smf.logit(formula=formula_n0, data=subset_n0).fit(disp=0)\n",
    "    save_path_n0 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_num_model_n0.pkl\")\n",
    "    joblib.dump(model_n0, save_path_n0)\n",
    "    trial[\"fitted_pp_switch_numerator_n0\"] = model_n0\n",
    "    \n",
    "    # Model d0: P(treatment = 1 | previous treatment = 0)\n",
    "    formula_d0 = \"treatment ~ age + x1 + x3\"\n",
    "    model_d0 = smf.logit(formula=formula_d0, data=subset_n0).fit(disp=0)\n",
    "    save_path_d0 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_den_model_d0.pkl\")\n",
    "    joblib.dump(model_d0, save_path_d0)\n",
    "    trial[\"fitted_pp_switch_denominator_d0\"] = model_d0\n",
    "    \n",
    "    return trial\n",
    "\n",
    "# Usage example for trial PP:\n",
    "# Ensure trial_pp has an assigned save_dir (e.g., within trial_pp_dir)\n",
    "trial_pp_switch[\"estimand\"] = \"PP\"\n",
    "trial_pp_switch[\"save_dir\"] = os.path.join(trial_pp_dir, \"switch_models\")\n",
    "os.makedirs(trial_pp_switch[\"save_dir\"], exist_ok=True)\n",
    "trial_pp_switch = calculate_pp_switch_weights(trial_pp_switch)\n",
    "\n",
    "# To verify, you can print summaries:\n",
    "print(\"Model n1 (Numerator for prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_numerator_n1\"].summary2().as_text())\n",
    "print(\"\\nModel d1 (Denom. for prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_denominator_d1\"].summary2().as_text())\n",
    "print(\"\\nModel n0 (Numerator for prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_numerator_n0\"].summary2().as_text())\n",
    "print(\"\\nModel d0 (Denom. for prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_denominator_d0\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Outcome Model\n",
    "Now we can specify the outcome model. Here we can include adjustment terms for any variables in the dataset. The numerator terms from the stabilised weight models are automatically included in the outcome model formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP Outcome Model Summary:\n",
      "                 Results: Ordinary least squares\n",
      "==================================================================\n",
      "Model:              OLS              Adj. R-squared:     0.001    \n",
      "Dependent Variable: outcome          AIC:                -987.8294\n",
      "Date:               2025-03-09 10:59 BIC:                -978.6570\n",
      "No. Observations:   725              Log-Likelihood:     495.91   \n",
      "Df Model:           1                F-statistic:        1.703    \n",
      "Df Residuals:       723              Prob (F-statistic): 0.192    \n",
      "R-squared:          0.002            Scale:              0.014948 \n",
      "--------------------------------------------------------------------\n",
      "             Coef.    Std.Err.      t      P>|t|     [0.025   0.975]\n",
      "--------------------------------------------------------------------\n",
      "Intercept    0.0207     0.0062    3.3304   0.0009    0.0085   0.0329\n",
      "treatment   -0.0119     0.0091   -1.3049   0.1923   -0.0297   0.0060\n",
      "------------------------------------------------------------------\n",
      "Omnibus:             1016.550     Durbin-Watson:        1.938     \n",
      "Prob(Omnibus):       0.000        Jarque-Bera (JB):     118616.404\n",
      "Skew:                7.904        Prob(JB):             0.000     \n",
      "Kurtosis:            63.636       Condition No.:        3         \n",
      "==================================================================\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the\n",
      "errors is correctly specified.\n",
      "\n",
      "ITT Outcome Model Summary:\n",
      "                 Results: Ordinary least squares\n",
      "==================================================================\n",
      "Model:              OLS              Adj. R-squared:     0.001    \n",
      "Dependent Variable: outcome          AIC:                -986.5142\n",
      "Date:               2025-03-09 10:59 BIC:                -972.7557\n",
      "No. Observations:   725              Log-Likelihood:     496.26   \n",
      "Df Model:           2                F-statistic:        1.192    \n",
      "Df Residuals:       722              Prob (F-statistic): 0.304    \n",
      "R-squared:          0.003            Scale:              0.014955 \n",
      "--------------------------------------------------------------------\n",
      "             Coef.    Std.Err.      t      P>|t|     [0.025   0.975]\n",
      "--------------------------------------------------------------------\n",
      "Intercept    0.0218     0.0064    3.4285   0.0006    0.0093   0.0343\n",
      "treatment   -0.0128     0.0092   -1.3947   0.1635   -0.0308   0.0052\n",
      "x2           0.0038     0.0046    0.8260   0.4091   -0.0052   0.0128\n",
      "------------------------------------------------------------------\n",
      "Omnibus:             1015.706     Durbin-Watson:        1.938     \n",
      "Prob(Omnibus):       0.000        Jarque-Bera (JB):     118171.416\n",
      "Skew:                7.893        Prob(JB):             0.000     \n",
      "Kurtosis:            63.520       Condition No.:        3         \n",
      "==================================================================\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the\n",
      "errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Function to set outcome model\n",
    "def set_outcome_model(trial, adjustment_terms=None):\n",
    "    # Connect outcome model specification with trial data after calculate_weights.\n",
    "    trial['outcome_model'] = {\n",
    "        \"formula\": \"outcome ~ assigned_treatment + x2 + followup_time + I(followup_time**2) + trial_period + I(trial_period**2)\",\n",
    "        \"adjustment_terms\": adjustment_terms,\n",
    "        \"fitted_model\": None  # to be set when fit_msm() is called\n",
    "    }\n",
    "    return trial\n",
    "\n",
    "# Apply outcome model for PP and ITT\n",
    "trial_pp = set_outcome_model(trial_pp)\n",
    "trial_itt = set_outcome_model(trial_itt, adjustment_terms=\"~x2\")\n",
    "\n",
    "# Print summaries to verify the fitting\n",
    "print(\"PP Outcome Model Summary:\")\n",
    "print(trial_pp[\"outcome_model\"].summary2().as_text())\n",
    "print(\"\\nITT Outcome Model Summary:\")\n",
    "print(trial_itt[\"outcome_model\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand Trials\n",
    "\n",
    "We prepare to create the dataset that includes the sequence of target trials. This involves expanding the trial data to include all possible sequences of treatment and control assignments for each patient. \n",
    "\n",
    "We use the `set_expansion_options` function to configure the expansion process. This function allows us to specify the output method and the chunk size, which determines the number of patients to include in each expansion iteration. \n",
    "\n",
    "For both the Per-protocol (PP) and Intention-to-treat (ITT) analyses, we set the output to a dummy function `save_to_datatable()` and the chunk size to 500 patients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(expanded_data)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m trial_pp_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mexpand_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_pp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m trial_itt_expanded \u001b[38;5;241m=\u001b[39m expand_trials(trial_itt, max_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(trial_pp_expanded\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[47], line 10\u001b[0m, in \u001b[0;36mexpand_trials\u001b[1;34m(trial, max_period)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexpand_trials\u001b[39m(trial, max_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtrial\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     11\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpansion_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m     12\u001b[0m     expanded_data \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "def set_expansion_options(trial, output, chunk_size):\n",
    "    trial[\"expansion_options\"] = {\n",
    "        \"output\": output,\n",
    "        \"chunk_size\": chunk_size\n",
    "    }\n",
    "    trial[\"data\"] = trial.get(\"data_with_weights\", trial[\"data\"])\n",
    "    return trial\n",
    "\n",
    "def expand_trials(trial, max_period=10):\n",
    "    data = trial[\"data\"].copy()\n",
    "    chunk_size = trial.get(\"expansion_options\", {}).get(\"chunk_size\", 500)\n",
    "    expanded_data = []\n",
    "    \n",
    "    for start in range(0, len(data), chunk_size):\n",
    "        chunk = data.iloc[start: start + chunk_size]\n",
    "        for _, row in chunk.iterrows():\n",
    "            for t in range(max_period + 1):\n",
    "                new_row = row.copy()\n",
    "                new_row[\"trial_period\"] = t\n",
    "                new_row[\"followup_time\"] = t\n",
    "                expanded_data.append(new_row)\n",
    "                \n",
    "    return pd.DataFrame(expanded_data)\n",
    "\n",
    "# Example usage\n",
    "trial_pp_expanded = expand_trials(trial_pp, max_period=10)\n",
    "trial_itt_expanded = expand_trials(trial_itt, max_period=10)\n",
    "print(trial_pp_expanded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
