{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight models calculated for ITT.\n",
      "Calculating final weights for ITT...\n",
      "Warning: Switch weights not set. Using default of 1.\n",
      "Final weights computed successfully.\n",
      "Weight Models for Informative Censoring (ITT)\n",
      "---------------------------------------------\n",
      "\n",
      "[[n]]\n",
      "                         Results: Logit\n",
      "================================================================\n",
      "Model:              Logit            Method:           MLE      \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.027    \n",
      "Date:               2025-03-02 19:19 AIC:              397.4004 \n",
      "No. Observations:   725              BIC:              406.5727 \n",
      "Df Model:           1                Log-Likelihood:   -196.70  \n",
      "Df Residuals:       723              LL-Null:          -202.11  \n",
      "Converged:          1.0000           LLR p-value:      0.0010067\n",
      "No. Iterations:     7.0000           Scale:            1.0000   \n",
      "------------------------------------------------------------------\n",
      "          Coef.    Std.Err.      z      P>|z|     [0.025    0.975]\n",
      "------------------------------------------------------------------\n",
      "const     2.4481     0.1406   17.4149   0.0000    2.1726    2.7236\n",
      "x2       -0.4486     0.1369   -3.2777   0.0010   -0.7169   -0.1804\n",
      "================================================================\n",
      "\n",
      "\n",
      "path\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\trial_itt\\switch_models\\model_numerator.pkl\n",
      "\n",
      "[[dd0]]\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.037   \n",
      "Date:               2025-03-02 19:19 AIC:              189.4423\n",
      "No. Observations:   386              BIC:              201.3098\n",
      "Df Model:           2                Log-Likelihood:   -91.721 \n",
      "Df Residuals:       383              LL-Null:          -95.245 \n",
      "Converged:          1.0000           LLR p-value:      0.029478\n",
      "No. Iterations:     11.0000          Scale:            1.0000  \n",
      "-----------------------------------------------------------------\n",
      "         Coef.    Std.Err.      z      P>|z|     [0.025    0.975]\n",
      "-----------------------------------------------------------------\n",
      "const    2.3596     0.2412    9.7840   0.0000    1.8869    2.8323\n",
      "x2      -0.5091     0.2220   -2.2932   0.0218   -0.9442   -0.0740\n",
      "x1       0.6687     0.4621    1.4472   0.1478   -0.2369    1.5744\n",
      "===============================================================\n",
      "\n",
      "\n",
      "path\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\trial_itt\\switch_models\\model_denominator_d0.pkl\n",
      "\n",
      "[[dd1]]\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.041   \n",
      "Date:               2025-03-02 19:19 AIC:              209.1639\n",
      "No. Observations:   339              BIC:              220.6419\n",
      "Df Model:           2                Log-Likelihood:   -101.58 \n",
      "Df Residuals:       336              LL-Null:          -105.97 \n",
      "Converged:          1.0000           LLR p-value:      0.012447\n",
      "No. Iterations:     10.0000          Scale:            1.0000  \n",
      "-----------------------------------------------------------------\n",
      "         Coef.    Std.Err.      z      P>|z|     [0.025    0.975]\n",
      "-----------------------------------------------------------------\n",
      "const    2.0371     0.2297    8.8666   0.0000    1.5868    2.4874\n",
      "x2      -0.4118     0.1784   -2.3081   0.0210   -0.7615   -0.0621\n",
      "x1       0.7549     0.4126    1.8296   0.0673   -0.0538    1.5635\n",
      "===============================================================\n",
      "\n",
      "\n",
      "path\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\trial_itt\\switch_models\\model_denominator_d1.pkl\n",
      "\n",
      "Switch weights successfully calculated.\n",
      "Weight models calculated for PP.\n",
      "Calculating final weights for PP...\n",
      "Final weights computed successfully.\n",
      "Weight Models for Informative Censoring (PP)\n",
      "--------------------------------------------\n",
      "\n",
      "[[nn0]]\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.043   \n",
      "Date:               2025-03-02 19:19 AIC:              87.1511 \n",
      "No. Observations:   97               BIC:              92.3005 \n",
      "Df Model:           1                Log-Likelihood:   -41.576 \n",
      "Df Residuals:       95               LL-Null:          -43.435 \n",
      "Converged:          1.0000           LLR p-value:      0.053788\n",
      "No. Iterations:     7.0000           Scale:            1.0000  \n",
      "-----------------------------------------------------------------\n",
      "          Coef.    Std.Err.      z      P>|z|     [0.025   0.975]\n",
      "-----------------------------------------------------------------\n",
      "const     1.6777     0.2898    5.7900   0.0000    1.1098   2.2456\n",
      "x2       -0.5999     0.3210   -1.8689   0.0616   -1.2290   0.0292\n",
      "===============================================================\n",
      "\n",
      "\n",
      "path\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\trial_pp\\switch_models\\model_numerator_n0.pkl\n",
      "\n",
      "[[nn1]]\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.040   \n",
      "Date:               2025-03-02 19:19 AIC:              84.3535 \n",
      "No. Observations:   73               BIC:              88.9344 \n",
      "Df Model:           1                Log-Likelihood:   -40.177 \n",
      "Df Residuals:       71               LL-Null:          -41.854 \n",
      "Converged:          1.0000           LLR p-value:      0.067017\n",
      "No. Iterations:     6.0000           Scale:            1.0000  \n",
      "-----------------------------------------------------------------\n",
      "          Coef.    Std.Err.      z      P>|z|     [0.025   0.975]\n",
      "-----------------------------------------------------------------\n",
      "const     1.0949     0.2785    3.9317   0.0001    0.5491   1.6407\n",
      "x2       -0.4901     0.2749   -1.7826   0.0747   -1.0289   0.0488\n",
      "===============================================================\n",
      "\n",
      "\n",
      "path\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\trial_pp\\switch_models\\model_numerator_n1.pkl\n",
      "\n",
      "[[dd0]]\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.037   \n",
      "Date:               2025-03-02 19:19 AIC:              189.4423\n",
      "No. Observations:   386              BIC:              201.3098\n",
      "Df Model:           2                Log-Likelihood:   -91.721 \n",
      "Df Residuals:       383              LL-Null:          -95.245 \n",
      "Converged:          1.0000           LLR p-value:      0.029478\n",
      "No. Iterations:     11.0000          Scale:            1.0000  \n",
      "-----------------------------------------------------------------\n",
      "         Coef.    Std.Err.      z      P>|z|     [0.025    0.975]\n",
      "-----------------------------------------------------------------\n",
      "const    2.3596     0.2412    9.7840   0.0000    1.8869    2.8323\n",
      "x2      -0.5091     0.2220   -2.2932   0.0218   -0.9442   -0.0740\n",
      "x1       0.6687     0.4621    1.4472   0.1478   -0.2369    1.5744\n",
      "===============================================================\n",
      "\n",
      "\n",
      "path\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\trial_pp\\switch_models\\model_denominator_d0.pkl\n",
      "\n",
      "[[dd1]]\n",
      "                        Results: Logit\n",
      "===============================================================\n",
      "Model:              Logit            Method:           MLE     \n",
      "Dependent Variable: not_censored     Pseudo R-squared: 0.041   \n",
      "Date:               2025-03-02 19:19 AIC:              209.1639\n",
      "No. Observations:   339              BIC:              220.6419\n",
      "Df Model:           2                Log-Likelihood:   -101.58 \n",
      "Df Residuals:       336              LL-Null:          -105.97 \n",
      "Converged:          1.0000           LLR p-value:      0.012447\n",
      "No. Iterations:     10.0000          Scale:            1.0000  \n",
      "-----------------------------------------------------------------\n",
      "         Coef.    Std.Err.      z      P>|z|     [0.025    0.975]\n",
      "-----------------------------------------------------------------\n",
      "const    2.0371     0.2297    8.8666   0.0000    1.5868    2.4874\n",
      "x2      -0.4118     0.1784   -2.3081   0.0210   -0.7615   -0.0621\n",
      "x1       0.7549     0.4126    1.8296   0.0673   -0.0538    1.5635\n",
      "===============================================================\n",
      "\n",
      "\n",
      "path\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\trial_pp\\switch_models\\model_denominator_d1.pkl\n",
      "\n",
      "Weight Models for Treatment Switching (PP)\n",
      "------------------------------------------\n",
      "\n",
      "[[n1]]\n",
      "                          Results: Logit\n",
      "==================================================================\n",
      "Model:              Logit            Method:           MLE        \n",
      "Dependent Variable: treatment        Pseudo R-squared: inf        \n",
      "Date:               2025-03-02 19:19 AIC:              4.0000     \n",
      "No. Observations:   339              BIC:              11.6520    \n",
      "Df Model:           1                Log-Likelihood:   -9.0811e-09\n",
      "Df Residuals:       337              LL-Null:          0.0000     \n",
      "Converged:          1.0000           LLR p-value:      1.0000     \n",
      "No. Iterations:     1.0000           Scale:            1.0000     \n",
      "------------------------------------------------------------------\n",
      "         Coef.    Std.Err.    z    P>|z|     [0.025       0.975]  \n",
      "------------------------------------------------------------------\n",
      "const    0.0222 161871.5422 0.0000 1.0000 -317262.3707 317262.4151\n",
      "age      0.9998   8176.4391 0.0001 0.9999  -16024.5264  16026.5259\n",
      "==================================================================\n",
      "\n",
      "\n",
      "path\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\trial_pp\\switching_models\\model_numerator_n1.pkl\n",
      "\n",
      "[[n0]]\n",
      "                          Results: Logit\n",
      "==================================================================\n",
      "Model:              Logit            Method:           MLE        \n",
      "Dependent Variable: treatment        Pseudo R-squared: inf        \n",
      "Date:               2025-03-02 19:19 AIC:              4.0000     \n",
      "No. Observations:   386              BIC:              11.9117    \n",
      "Df Model:           1                Log-Likelihood:   -8.8078e-09\n",
      "Df Residuals:       384              LL-Null:          0.0000     \n",
      "Converged:          1.0000           LLR p-value:      1.0000     \n",
      "No. Iterations:     1.0000           Scale:            1.0000     \n",
      "------------------------------------------------------------------\n",
      "        Coef.    Std.Err.     z    P>|z|     [0.025       0.975]  \n",
      "------------------------------------------------------------------\n",
      "const  -0.0197 183248.8282 -0.0000 1.0000 -359161.1231 359161.0837\n",
      "age    -0.9998   9289.2775 -0.0001 0.9999  -18207.6492  18205.6496\n",
      "==================================================================\n",
      "\n",
      "\n",
      "path\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\trial_pp\\switching_models\\model_numerator_n0.pkl\n",
      "\n",
      "[[d1]]\n",
      "                          Results: Logit\n",
      "==================================================================\n",
      "Model:              Logit            Method:           MLE        \n",
      "Dependent Variable: treatment        Pseudo R-squared: inf        \n",
      "Date:               2025-03-02 19:19 AIC:              8.0000     \n",
      "No. Observations:   339              BIC:              23.3040    \n",
      "Df Model:           3                Log-Likelihood:   -9.0541e-09\n",
      "Df Residuals:       335              LL-Null:          0.0000     \n",
      "Converged:          1.0000           LLR p-value:      1.0000     \n",
      "No. Iterations:     1.0000           Scale:            1.0000     \n",
      "------------------------------------------------------------------\n",
      "         Coef.    Std.Err.    z    P>|z|     [0.025       0.975]  \n",
      "------------------------------------------------------------------\n",
      "const    0.0222 208475.5294 0.0000 1.0000 -408604.5071 408604.5515\n",
      "age      0.9996  10866.2341 0.0001 0.9999  -21296.4278  21298.4271\n",
      "x1       0.0096  40542.2355 0.0000 1.0000  -79461.3118  79461.3310\n",
      "x3       0.0110  28090.9111 0.0000 1.0000  -55057.1629  55057.1850\n",
      "==================================================================\n",
      "\n",
      "\n",
      "path\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\trial_pp\\switching_models\\model_denominator_d1.pkl\n",
      "\n",
      "[[d0]]\n",
      "                          Results: Logit\n",
      "==================================================================\n",
      "Model:              Logit            Method:           MLE        \n",
      "Dependent Variable: treatment        Pseudo R-squared: inf        \n",
      "Date:               2025-03-02 19:19 AIC:              8.0000     \n",
      "No. Observations:   386              BIC:              23.8233    \n",
      "Df Model:           3                Log-Likelihood:   -8.7437e-09\n",
      "Df Residuals:       382              LL-Null:          0.0000     \n",
      "Converged:          1.0000           LLR p-value:      1.0000     \n",
      "No. Iterations:     1.0000           Scale:            1.0000     \n",
      "------------------------------------------------------------------\n",
      "        Coef.    Std.Err.     z    P>|z|     [0.025       0.975]  \n",
      "------------------------------------------------------------------\n",
      "const  -0.0197 342266.1823 -0.0000 1.0000 -670829.4101 670829.3707\n",
      "age    -0.9997  15424.6942 -0.0001 0.9999  -30232.8449  30230.8454\n",
      "x1     -0.0075  73941.9041 -0.0000 1.0000 -144923.4765 144923.4615\n",
      "x3     -0.0094  55544.5259 -0.0000 1.0000 -108865.2797 108865.2609\n",
      "==================================================================\n",
      "\n",
      "\n",
      "path\n",
      "C:\\Users\\USER\\Documents\\3rd year 2nd sem\\Data Analytics\\Assignments_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\trial_pp\\switching_models\\model_denominator_d0.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import (\n",
    "    PerfectSeparationWarning, \n",
    "    HessianInversionWarning\n",
    ")\n",
    "\n",
    "# Suppress all relevant warnings\n",
    "warnings.filterwarnings('ignore', category=PerfectSeparationWarning)\n",
    "warnings.filterwarnings('ignore', category=HessianInversionWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Function to create directories\n",
    "def create_trial_directory(dir_name):\n",
    "    dir_path = os.path.join(os.getcwd(), dir_name)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    return dir_path\n",
    "\n",
    "# Creating directories\n",
    "trial_pp_dir = create_trial_directory(\"trial_pp\")\n",
    "trial_itt_dir = create_trial_directory(\"trial_itt\")\n",
    "\n",
    "# Load dataset\n",
    "data_censored = pd.read_csv('c:/Users/USER/Documents/3rd year 2nd sem/Data Analytics/Assignments_Data_Analytics/Assignment_1_Clustering_Data_Analytics/data_censored.csv')\n",
    "\n",
    "# Define logistic model fitting function with improved stability\n",
    "def fit_logistic_model(data, predictors, outcome):\n",
    "    X = sm.add_constant(data[predictors])\n",
    "    y = data[outcome]\n",
    "    try:\n",
    "        model = sm.Logit(y, X).fit(disp=0, method=\"lbfgs\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Logistic model fitting failed for {outcome} with error: {e}\")\n",
    "        return None\n",
    "    return model\n",
    "\n",
    "class TrialSequence:\n",
    "    def __init__(self, estimand, data=None):\n",
    "        self.estimand = estimand\n",
    "        self.data = data\n",
    "        self.switch_weights = None\n",
    "        self.censor_weight_model = None\n",
    "        self.weights = {}         # For informative censoring models\n",
    "        self.switch_models = {}   # For treatment switching models\n",
    "\n",
    "    def set_data(self, data, id_col, period_col, treatment_col, outcome_col, eligible_col):\n",
    "        self.data = data[[id_col, period_col, treatment_col, outcome_col, eligible_col,\n",
    "                          'x1', 'x2', 'x3', 'x4', 'age', 'age_s', 'censored']].copy()\n",
    "\n",
    "    def set_switch_weight_model(self, numerator_predictors, denominator_predictors):\n",
    "        # This method is only applicable for PP estimand.\n",
    "        if self.estimand != \"PP\":\n",
    "            print(\"Switch weight model is only applicable to PP estimand.\")\n",
    "            return\n",
    "\n",
    "        data = self.data.copy()\n",
    "\n",
    "        # Split data by previous treatment for treatment switching models.\n",
    "        data_t1 = data[data['treatment'] == 1].copy()  # previous treatment = 1\n",
    "        data_t0 = data[data['treatment'] == 0].copy()  # previous treatment = 0\n",
    "\n",
    "        # Fit numerator models\n",
    "        num_model_t1 = fit_logistic_model(data_t1, numerator_predictors, outcome='treatment')\n",
    "        num_model_t0 = fit_logistic_model(data_t0, numerator_predictors, outcome='treatment')\n",
    "\n",
    "        # Fit denominator models\n",
    "        denom_model_t1 = fit_logistic_model(data_t1, denominator_predictors, outcome='treatment')\n",
    "        denom_model_t0 = fit_logistic_model(data_t0, denominator_predictors, outcome='treatment')\n",
    "\n",
    "        if any(m is None for m in [num_model_t1, num_model_t0, denom_model_t1, denom_model_t0]):\n",
    "            print(\"Error: One or more switching models failed to fit. Skipping switch weight computation.\")\n",
    "            return\n",
    "\n",
    "        # Save the switching models\n",
    "        self.switch_models['numerator_n1'] = num_model_t1\n",
    "        self.switch_models['numerator_n0'] = num_model_t0\n",
    "        self.switch_models['denominator_d1'] = denom_model_t1\n",
    "        self.switch_models['denominator_d0'] = denom_model_t0\n",
    "\n",
    "        # Compute predicted probabilities separately for each group\n",
    "        data['num_prob'] = 0\n",
    "        data['denom_prob'] = 0\n",
    "        for grp in [1, 0]:\n",
    "            subset = data[data['treatment'] == grp]\n",
    "            if grp == 1:\n",
    "                X_num = sm.add_constant(subset[numerator_predictors])\n",
    "                X_denom = sm.add_constant(subset[denominator_predictors])\n",
    "                data.loc[subset.index, 'num_prob'] = num_model_t1.predict(X_num).astype(float)\n",
    "                data.loc[subset.index, 'denom_prob'] = denom_model_t1.predict(X_denom).astype(float)\n",
    "            else:\n",
    "                X_num = sm.add_constant(subset[numerator_predictors])\n",
    "                X_denom = sm.add_constant(subset[denominator_predictors])\n",
    "                data.loc[subset.index, 'num_prob'] = num_model_t0.predict(X_num).astype(float)\n",
    "                data.loc[subset.index, 'denom_prob'] = denom_model_t0.predict(X_denom).astype(float)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        data['denom_prob'] = np.clip(data['denom_prob'], 1e-6, 1)\n",
    "\n",
    "        data['switch_weight'] = data['num_prob'] / data['denom_prob']\n",
    "        data['switch_weight'] = np.clip(data['switch_weight'], 0.01, 10)\n",
    "\n",
    "        self.switch_weights = data[[\"id\", \"period\", \"treatment\", \"switch_weight\"]]\n",
    "        print(\"Switch weights successfully calculated.\")\n",
    "\n",
    "    def set_censor_weight_model(self, censor_event, numerator, denominator):\n",
    "        self.censor_weight_model = {'censor_event': censor_event, 'numerator': numerator, 'denominator': denominator}\n",
    "\n",
    "    def calculate_weights(self):\n",
    "        if self.censor_weight_model:\n",
    "            self.fit_censoring_models()\n",
    "        print(f\"Weight models calculated for {self.estimand}.\")\n",
    "\n",
    "        if self.data is None:\n",
    "            print(\"Error: No data available. Please set data using set_data().\")\n",
    "            return\n",
    "\n",
    "        print(f\"Calculating final weights for {self.estimand}...\")\n",
    "\n",
    "        # Step 1: Apply switch weights (Treatment Switching)\n",
    "        if self.switch_weights is not None:\n",
    "            self.data = self.data.merge(self.switch_weights, on=[\"id\", \"period\", \"treatment\"], how=\"left\")\n",
    "        else:\n",
    "            print(\"Warning: Switch weights not set. Using default of 1.\")\n",
    "            self.data[\"switch_weight\"] = 1\n",
    "\n",
    "        # Step 2: Compute Censoring Weights (Inverse Probability of Remaining Uncensored)\n",
    "        self.data[\"not_censored\"] = 1 - self.data[\"censored\"]\n",
    "\n",
    "        # For censoring, compute predicted probabilities for numerator and denominator by treatment\n",
    "        if self.estimand == \"PP\":\n",
    "            # For PP, use separate models for treatment 0 and 1\n",
    "            for grp in [0, 1]:\n",
    "                idx = self.data[self.data[\"treatment\"] == grp].index\n",
    "                num_model = self.weights.get(f\"numerator_n{grp}\")\n",
    "                den_model = self.weights.get(f\"denominator_d{grp}\")\n",
    "                if num_model is not None and den_model is not None:\n",
    "                    X_num = sm.add_constant(self.data.loc[idx, ['x2']])\n",
    "                    X_den = sm.add_constant(self.data.loc[idx, ['x2', 'x1']])\n",
    "                    self.data.loc[idx, \"numerator\"] = num_model.predict(X_num).astype(float)\n",
    "                    self.data.loc[idx, \"denominator\"] = den_model.predict(X_den).astype(float)\n",
    "                else:\n",
    "                    self.data.loc[idx, \"numerator\"] = 1\n",
    "                    self.data.loc[idx, \"denominator\"] = 1\n",
    "        else:\n",
    "            # ITT: Use the overall numerator model (if available) and group‚Äêwise denominator models\n",
    "            idx_all = self.data.index\n",
    "            num_model = self.weights.get(\"numerator\")\n",
    "            if num_model is not None:\n",
    "                X_num = sm.add_constant(self.data.loc[idx_all, ['x2']])\n",
    "                self.data.loc[idx_all, \"numerator\"] = num_model.predict(X_num).astype(float)\n",
    "            else:\n",
    "                self.data.loc[idx_all, \"numerator\"] = 1\n",
    "            for grp in [0, 1]:\n",
    "                idx = self.data[self.data[\"treatment\"] == grp].index\n",
    "                den_model = self.weights.get(f\"denominator_d{grp}\")\n",
    "                if den_model is not None:\n",
    "                    X_den = sm.add_constant(self.data.loc[idx, ['x2', 'x1']])\n",
    "                    self.data.loc[idx, \"denominator\"] = den_model.predict(X_den).astype(float)\n",
    "                else:\n",
    "                    self.data.loc[idx, \"denominator\"] = 1\n",
    "\n",
    "        # Avoid division by zero and compute censoring weight\n",
    "        self.data[\"denominator\"] = np.clip(self.data[\"denominator\"], 1e-6, 1)\n",
    "        self.data[\"censor_weight\"] = self.data[\"numerator\"] / self.data[\"denominator\"]\n",
    "        self.data[\"censor_weight\"] = np.clip(self.data[\"censor_weight\"], 0.01, 10)\n",
    "\n",
    "        # Step 3: Compute Final Stabilized Weights\n",
    "        self.data[\"final_weight\"] = self.data[\"switch_weight\"] * self.data[\"censor_weight\"]\n",
    "        print(\"Final weights computed successfully.\")\n",
    "\n",
    "    def fit_censoring_models(self):\n",
    "        self.data[\"not_censored\"] = 1 - self.data[\"censored\"]\n",
    "        if self.estimand == \"PP\":\n",
    "            data_n0 = self.data[(self.data['treatment'] == 0) & (self.data['eligible'] == 1)].copy()\n",
    "            data_n1 = self.data[(self.data['treatment'] == 1) & (self.data['eligible'] == 1)].copy()\n",
    "            self.weights['numerator_n0'] = fit_logistic_model(data_n0, ['x2'], outcome='not_censored')\n",
    "            self.weights['numerator_n1'] = fit_logistic_model(data_n1, ['x2'], outcome='not_censored')\n",
    "        else:\n",
    "            self.weights['numerator'] = fit_logistic_model(self.data, ['x2'], outcome='not_censored')\n",
    "        data_d0 = self.data[self.data['treatment'] == 0].copy()\n",
    "        data_d1 = self.data[self.data['treatment'] == 1].copy()\n",
    "        self.weights['denominator_d0'] = fit_logistic_model(data_d0, ['x2', 'x1'], outcome='not_censored')\n",
    "        self.weights['denominator_d1'] = fit_logistic_model(data_d1, ['x2', 'x1'], outcome='not_censored')\n",
    "\n",
    "# Function to display and save weight model summaries in the trialemulation style\n",
    "def show_weight_models(trial):\n",
    "    # Determine readable header based on estimand type\n",
    "    if trial.estimand == \"ITT\":\n",
    "        header = \"Weight Models for Informative Censoring (ITT)\"\n",
    "    elif trial.estimand == \"PP\":\n",
    "        header = \"Weight Models for Informative Censoring (PP)\"\n",
    "    else:\n",
    "        header = \"Weight Models for Informative Censoring\"\n",
    "        \n",
    "    print(header)\n",
    "    print(\"-\" * len(header) + \"\\n\")\n",
    "    \n",
    "    for key, model in trial.weights.items():\n",
    "        if model is None:\n",
    "            print(f\"Warning: Model for key '{key}' is not available.\\n\")\n",
    "            continue\n",
    "\n",
    "        # For PP, label using the group indicator (e.g., n0, n1, d0, d1)\n",
    "        if trial.estimand == \"PP\":\n",
    "            if key.startswith(\"numerator_n\"):\n",
    "                label = \"n\" + key.split(\"_\")[-1]\n",
    "            elif key.startswith(\"denominator_d\"):\n",
    "                label = \"d\" + key.split(\"_\")[-1]\n",
    "            else:\n",
    "                label = key\n",
    "        else:\n",
    "            # For ITT, label numerator as 'n' and denominator as 'd{grp}'\n",
    "            if key == \"numerator\":\n",
    "                label = \"n\"\n",
    "            elif key.startswith(\"denominator_d\"):\n",
    "                label = \"d\" + key.split(\"_\")[-1]\n",
    "            else:\n",
    "                label = key\n",
    "\n",
    "        print(f\"[[{label}]]\")\n",
    "        try:\n",
    "            summary_text = model.summary2().as_text()\n",
    "        except Exception:\n",
    "            summary_text = model.summary().as_text()\n",
    "        print(summary_text + \"\\n\")\n",
    "\n",
    "        # Save the model object to disk in a subfolder \"switch_models\"\n",
    "        dir_path = trial_pp_dir if trial.estimand == \"PP\" else trial_itt_dir\n",
    "        model_dir = os.path.join(dir_path, \"switch_models\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        model_filename = f\"model_{key}.pkl\"\n",
    "        model_path = os.path.join(model_dir, model_filename)\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(\"path\")\n",
    "        print(model_path + \"\\n\")\n",
    "    \n",
    "    # For PP, also display Treatment Switching Models\n",
    "    if trial.estimand == \"PP\" and trial.switch_models:\n",
    "        header_ts = \"Weight Models for Treatment Switching (PP)\"\n",
    "        print(header_ts)\n",
    "        print(\"-\" * len(header_ts) + \"\\n\")\n",
    "        for key, model in trial.switch_models.items():\n",
    "            if model is None:\n",
    "                print(f\"Warning: Model for key '{key}' is not available.\\n\")\n",
    "                continue\n",
    "            label = key.split(\"_\")[-1]  # e.g. n0, n1, d0, d1\n",
    "            print(f\"[[{label}]]\")\n",
    "            try:\n",
    "                summary_text = model.summary2().as_text()\n",
    "            except Exception:\n",
    "                summary_text = model.summary().as_text()\n",
    "            print(summary_text + \"\\n\")\n",
    "\n",
    "            # Save the switching model object to disk in a separate folder.\n",
    "            dir_path = trial_pp_dir if trial.estimand == \"PP\" else trial_itt_dir\n",
    "            model_dir = os.path.join(dir_path, \"switching_models\")\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            model_filename = f\"model_{key}.pkl\"\n",
    "            model_path = os.path.join(model_dir, model_filename)\n",
    "            with open(model_path, \"wb\") as f:\n",
    "                pickle.dump(model, f)\n",
    "            print(\"path\")\n",
    "            print(model_path + \"\\n\")\n",
    "\n",
    "# Process ITT trial\n",
    "trial_itt = TrialSequence(\"ITT\")\n",
    "trial_itt.set_data(data_censored, \"id\", \"period\", \"treatment\", \"outcome\", \"eligible\")\n",
    "trial_itt.set_censor_weight_model(\"censored\", [\"x2\"], [\"x2\", \"x1\"])\n",
    "trial_itt.calculate_weights()\n",
    "\n",
    "# Display weight model summaries for ITT trial\n",
    "show_weight_models(trial_itt)\n",
    "\n",
    "# Process PP trial\n",
    "trial_pp = TrialSequence(\"PP\")\n",
    "trial_pp.set_data(data_censored, \"id\", \"period\", \"treatment\", \"outcome\", \"eligible\")\n",
    "trial_pp.set_switch_weight_model([\"age\"], [\"age\", \"x1\", \"x3\"])\n",
    "trial_pp.set_censor_weight_model(\"censored\", [\"x2\"], [\"x2\", \"x1\"])\n",
    "trial_pp.calculate_weights()\n",
    "\n",
    "# Display weight model summaries for PP trial\n",
    "show_weight_models(trial_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
