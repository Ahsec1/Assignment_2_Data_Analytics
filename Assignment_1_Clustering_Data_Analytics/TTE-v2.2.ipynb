{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup \n",
    "\n",
    "In this section we define our target trial estimands for two scenarios:\n",
    "- **Per-protocol (PP):** Focused on patients adhering strictly to the treatment protocol.\n",
    "- **Intention-to-treat (ITT):** Analyses based on the treatment as assigned regardless of adherence.\n",
    "\n",
    "We also create directories using Pythonâ€™s `tempfile` module to store model outputs or intermediate files for later inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "# Define the estimands\n",
    "estimand_pp = \"PP\"  # Per-protocol\n",
    "estimand_itt = \"ITT\"  # Intention-to-treat\n",
    "\n",
    "# Create directories to save files for later inspection\n",
    "trial_pp_dir = os.path.join(tempfile.gettempdir(), \"trial_pp\")\n",
    "os.makedirs(trial_pp_dir, exist_ok=True)\n",
    "\n",
    "trial_itt_dir = os.path.join(tempfile.gettempdir(), \"trial_itt\")\n",
    "os.makedirs(trial_itt_dir, exist_ok=True)\n",
    "\n",
    "# The data_censored.csv file will be used later for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Clustering before assigning treatments\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Data/data_censored.csv')\n",
    "\n",
    "# Select features for clustering (excluding non-numeric or ID columns)\n",
    "features = df[['x1', 'x2', 'x3', 'x4', 'age']]\n",
    "\n",
    "# Apply K-Means clustering\n",
    "num_clusters = 3  # You can change this based on analysis\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df['Cluster_ID'] = kmeans.fit_predict(features)\n",
    "\n",
    "# Save modified dataset with Cluster_ID\n",
    "df.to_csv('data_clustered.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: Treatment Effect Estimate\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                outcome   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.005\n",
      "Method:                 Least Squares   F-statistic:                   0.01720\n",
      "Date:                Sun, 09 Mar 2025   Prob (F-statistic):              0.896\n",
      "Time:                        19:56:47   Log-Likelihood:                 128.34\n",
      "No. Observations:                 192   AIC:                            -252.7\n",
      "Df Residuals:                     190   BIC:                            -246.2\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0141      0.015      0.952      0.342      -0.015       0.043\n",
      "treatment      0.0024      0.019      0.131      0.896      -0.034       0.039\n",
      "==============================================================================\n",
      "Omnibus:                      308.021   Durbin-Watson:                   2.030\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            29804.851\n",
      "Skew:                           7.810   Prob(JB):                         0.00\n",
      "Kurtosis:                      62.005   Cond. No.                         3.05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "============================================================\n",
      "Cluster 0: Treatment Effect Estimate\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                outcome   R-squared:                       0.002\n",
      "Model:                            OLS   Adj. R-squared:                 -0.001\n",
      "Method:                 Least Squares   F-statistic:                    0.7526\n",
      "Date:                Sun, 09 Mar 2025   Prob (F-statistic):              0.386\n",
      "Time:                        19:56:47   Log-Likelihood:                 266.12\n",
      "No. Observations:                 333   AIC:                            -528.2\n",
      "Df Residuals:                     331   BIC:                            -520.6\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0169      0.008      2.060      0.040       0.001       0.033\n",
      "treatment     -0.0104      0.012     -0.868      0.386      -0.034       0.013\n",
      "==============================================================================\n",
      "Omnibus:                      536.020   Durbin-Watson:                   2.006\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            88632.420\n",
      "Skew:                           8.928   Prob(JB):                         0.00\n",
      "Kurtosis:                      80.904   Cond. No.                         2.55\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "============================================================\n",
      "Cluster 2: Treatment Effect Estimate\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                outcome   R-squared:                       0.009\n",
      "Model:                            OLS   Adj. R-squared:                  0.004\n",
      "Method:                 Least Squares   F-statistic:                     1.876\n",
      "Date:                Sun, 09 Mar 2025   Prob (F-statistic):              0.172\n",
      "Time:                        19:56:47   Log-Likelihood:                 110.38\n",
      "No. Observations:                 200   AIC:                            -216.8\n",
      "Df Residuals:                     198   BIC:                            -210.2\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0292      0.012      2.440      0.016       0.006       0.053\n",
      "treatment     -0.0292      0.021     -1.370      0.172      -0.071       0.013\n",
      "==============================================================================\n",
      "Omnibus:                      289.584   Durbin-Watson:                   1.770\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            17746.685\n",
      "Skew:                           6.758   Prob(JB):                         0.00\n",
      "Kurtosis:                      47.124   Cond. No.                         2.42\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Analyze treatment effects within each cluster\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load clustered data\n",
    "df = pd.read_csv('data_clustered.csv')\n",
    "\n",
    "# Define function to estimate treatment effect per cluster\n",
    "def analyze_cluster_treatment_effect(df, cluster_id):\n",
    "    cluster_df = df[df['Cluster_ID'] == cluster_id]\n",
    "    \n",
    "    # Define treatment and outcome variables (modify based on dataset structure)\n",
    "    X = sm.add_constant(cluster_df[['treatment']])  # Predictor: treatment\n",
    "    y = cluster_df['outcome']  # Outcome variable (modify accordingly)\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()\n",
    "    print(f\"Cluster {cluster_id}: Treatment Effect Estimate\")\n",
    "    print(model.summary())\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Apply function to each cluster\n",
    "for cluster in df['Cluster_ID'].unique():\n",
    "    analyze_cluster_treatment_effect(df, cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In this section we load the observational data from the `data_censored.csv` file which will be used for the target trial emulation. \n",
    "The dataset includes columns such as `id`, `period`, `treatment`, `x1`, `x2`, `x3`, `x4`, `age`, `age_s`, `outcome`, `censored`, and `eligible`.\n",
    "\n",
    "We then define a helper function `set_data` to associate specific columns with their roles in the trial data. \n",
    "\n",
    "For the Per-protocol analysis, the dataset is assigned to the `trial_pp` object using a pipe-like style, while for the ITT analysis, a standard function call is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Github\\\\Assignment_2_Data_Analytics\\\\Assignment_1_Clustering_Data_Analytics\\\\Data\\\\data_censored.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the observational data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data_censored \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mGithub\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mAssignment_2_Data_Analytics\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mAssignment_1_Clustering_Data_Analytics\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdata_censored.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_censored\u001b[38;5;241m.\u001b[39mhead())  \u001b[38;5;66;03m# display first few rows\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Github\\\\Assignment_2_Data_Analytics\\\\Assignment_1_Clustering_Data_Analytics\\\\Data\\\\data_censored.csv'"
     ]
    }
   ],
   "source": [
    "# Load the observational data\n",
    "data_censored = pd.read_csv('C:\\\\Github\\\\Assignment_2_Data_Analytics\\\\Assignment_1_Clustering_Data_Analytics\\\\Data\\\\data_censored.csv')\n",
    "print(data_censored.head())  # display first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_censored' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define Per-protocol (PP) dataset with data included\u001b[39;00m\n\u001b[0;32m      2\u001b[0m trial_pp \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdata_censored\u001b[49m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperiod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperiod\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtreatment\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtreatment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutcome\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutcome\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meligible\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meligible\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m }\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Define Intention-to-Treat (ITT) dataset with data included\u001b[39;00m\n\u001b[0;32m     12\u001b[0m trial_itt \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_censored,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meligible\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meligible\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_censored' is not defined"
     ]
    }
   ],
   "source": [
    "# Define Per-protocol (PP) dataset with data included\n",
    "trial_pp = {\n",
    "    \"data\": data_censored,\n",
    "    \"id\": \"id\",\n",
    "    \"period\": \"period\",\n",
    "    \"treatment\": \"treatment\",\n",
    "    \"outcome\": \"outcome\",\n",
    "    \"eligible\": \"eligible\"\n",
    "}\n",
    "\n",
    "# Define Intention-to-Treat (ITT) dataset with data included\n",
    "trial_itt = {\n",
    "    \"data\": data_censored,\n",
    "    \"id\": \"id\",\n",
    "    \"period\": \"period\",\n",
    "    \"treatment\": \"treatment\",\n",
    "    \"outcome\": \"outcome\",\n",
    "    \"eligible\": \"eligible\"\n",
    "}\n",
    "\n",
    "# Compute total observations and unique patients\n",
    "n_obs = len(data_censored)\n",
    "n_patients = data_censored['id'].nunique()\n",
    "\n",
    "# Get the first 2 rows and last 2 rows of the data\n",
    "head_df = data_censored.head(2)\n",
    "tail_df = data_censored.tail(2)\n",
    "\n",
    "def print_data_showcase(data, estimand_label):\n",
    "    # Compute total observations and unique patients\n",
    "    n_obs = len(data)\n",
    "    n_patients = data['id'].nunique()\n",
    "    \n",
    "    # Get the first 2 rows and last 2 rows of the data\n",
    "    head_df = data.head(2)\n",
    "    tail_df = data.tail(2)\n",
    "    \n",
    "    # Manually construct header strings with column names and types (as in provided example)\n",
    "    print(\"Trial Sequence Object\")\n",
    "    print(\"Estimand: \" + estimand_label)\n",
    "    print(\"\\nData:\")\n",
    "    print(\"  - N: {} observations from {} patients\".format(n_obs, n_patients))\n",
    "    print(\"         id period treatment    x1           x2   x3        x4   age      age_s\")\n",
    "    print(\"      <int> <int>     <num> <num>        <num> <int>     <num> <num>      <num>\")\n",
    "    print(head_df.to_string(index=True))\n",
    "    print(\"---\")\n",
    "    print(tail_df.to_string(index=True))\n",
    "    # For the outcome part, print outcome, censored, eligible columns similarly:\n",
    "    print(\"\\n      outcome censored eligible\")\n",
    "    print(\"        <num>    <int>    <num>\")\n",
    "    head_outcome = head_df[[\"outcome\", \"censored\", \"eligible\"]]\n",
    "    tail_outcome = tail_df[[\"outcome\", \"censored\", \"eligible\"]]\n",
    "    print(head_outcome.to_string(index=True))\n",
    "    print(\"---\")\n",
    "    print(tail_outcome.to_string(index=True))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print showcase for Per-protocol (PP) trial\n",
    "print_data_showcase(trial_pp[\"data\"], \"Per-protocol\")\n",
    "\n",
    "# Print showcase for Intention-to-treat (ITT) trial\n",
    "print_data_showcase(trial_itt[\"data\"], \"Intention-to-treat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Models and Censoring\n",
    "\n",
    "In this step we adjust for informative censoring by applying inverse probability of censoring weights (IPCW). Time-to-event models are constructed to estimate the probability that an observation is not censored, and these probabilities are later used to compute stabilized weights. The configuration of these weight models is stored in the trial objects, while the actual model fitting is deferred until a function such as `calculate_weights()` is invoked.\n",
    "\n",
    "- **Censoring Due to Treatment Switching (PP only):**  \n",
    "  For the Per-protocol estimand, separate models are specified for the numerator (using a limited set of covariates such as age) and the denominator (using an extended set like age, x1, and x3). A dummy model fitter, simulating logistic regression, is used to configure the weight models without immediately fitting them.\n",
    "\n",
    "- **Other Informative Censoring:**  \n",
    "  For both PP and ITT, models are defined to estimate the probability of remaining uncensored. This involves specifying the censoring event (e.g., the \"censored\" column) along with numerator and denominator models (e.g., using x2 in the numerator vs. x2 + x1 in the denominator) and an option to pool models. The configurations are stored, and the models are fit later when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_pp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trial\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Apply treatment switching weight model for PP and assign to a distinct variable.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m trial_pp_switch \u001b[38;5;241m=\u001b[39m set_switch_weight_model(\n\u001b[1;32m---> 32\u001b[0m     \u001b[43mtrial_pp\u001b[49m,\n\u001b[0;32m     33\u001b[0m     numerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m     denominator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage + x1 + x3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     model_fitter\u001b[38;5;241m=\u001b[39mStatsGLMLogit(save_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(trial_pp_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswitch_models\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrial_pp_switch weights config (treatment switching):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(trial_pp_switch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswitch_weights_config\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trial_pp' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a dummy model fitter to simulate fitting using logistic regression\n",
    "class StatsGLMLogit:\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "    def __repr__(self):\n",
    "        return f\"te_stats_glm_logit (save_path={self.save_path})\"\n",
    "\n",
    "# Function to set switch weight model (used only for PP)\n",
    "def set_switch_weight_model(trial, numerator, denominator, model_fitter):\n",
    "    trial[\"switch_weights_config\"] = {\n",
    "         \"numerator_formula\": f\"treatment ~ {numerator}\",\n",
    "         \"denominator_formula\": f\"treatment ~ {denominator}\",\n",
    "         \"model_fitter\": model_fitter,\n",
    "         \"note\": \"Weight models not fitted. Use calculate_weights()\"\n",
    "    }\n",
    "    return trial\n",
    "\n",
    "# Function to set censor weight model for informative censoring\n",
    "def set_censor_weight_model(trial, censor_event, numerator, denominator, pool_models, model_fitter):\n",
    "    trial[\"censor_weights_config\"] = {\n",
    "         \"censor_event\": censor_event,\n",
    "         \"numerator_formula\": f\"1 - {censor_event} ~ {numerator}\",\n",
    "         \"denominator_formula\": f\"1 - {censor_event} ~ {denominator}\",\n",
    "         \"pool_models\": pool_models,\n",
    "         \"model_fitter\": model_fitter,\n",
    "         \"note\": \"Weight models not fitted. Use calculate_weights()\"\n",
    "    }\n",
    "    return trial\n",
    "\n",
    "# Apply treatment switching weight model for PP and assign to a distinct variable.\n",
    "trial_pp_switch = set_switch_weight_model(\n",
    "    trial_pp,\n",
    "    numerator=\"age\",\n",
    "    denominator=\"age + x1 + x3\",\n",
    "    model_fitter=StatsGLMLogit(save_path=os.path.join(trial_pp_dir, \"switch_models\"))\n",
    ")\n",
    "print(\"trial_pp_switch weights config (treatment switching):\")\n",
    "print(trial_pp_switch[\"switch_weights_config\"])\n",
    "\n",
    "# Apply censor weight model on a copy of PP to keep it separate.\n",
    "trial_pp_censor = set_censor_weight_model(\n",
    "    trial_pp.copy(),\n",
    "    censor_event=\"censored\",\n",
    "    numerator=\"x2\",\n",
    "    denominator=\"x2 + x1\",\n",
    "    pool_models=\"none\",\n",
    "    model_fitter=StatsGLMLogit(save_path=os.path.join(trial_pp_dir, \"censor_models\"))\n",
    ")\n",
    "print(\"trial_pp_censor weights config (censoring):\")\n",
    "print(trial_pp_censor[\"censor_weights_config\"])\n",
    "\n",
    "# For ITT, censoring weights remain as before.\n",
    "trial_itt = set_censor_weight_model(\n",
    "    trial_itt,\n",
    "    censor_event=\"censored\",\n",
    "    numerator=\"x2\",\n",
    "    denominator=\"x2 + x1\",\n",
    "    pool_models=\"numerator\",\n",
    "    model_fitter=StatsGLMLogit(save_path=os.path.join(trial_itt_dir, \"censor_models\"))\n",
    ")\n",
    "print(\"trial_itt censor_weights_config:\")\n",
    "print(trial_itt[\"censor_weights_config\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Weights\n",
    "\n",
    "In this step we fit the individual models that were configured in Step 3 and then combine them into inverse probability of censoring weights (IPCW). The function `calculate_weights()` is used to perform the model fitting. The fitted model objects are saved on disk in the directories we created earlier, and the weight model summaries are stored in the trial sequence objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_itt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trial\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# --- Compute ITT weights ---\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m trial_itt \u001b[38;5;241m=\u001b[39m calculate_itt_weights(\u001b[43mtrial_itt\u001b[49m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# --- Define the path to save the CSV file ---\u001b[39;00m\n\u001b[0;32m     50\u001b[0m data_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGithub\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAssignment_2_Data_Analytics\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAssignment_1_Clustering_Data_Analytics\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trial_itt' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import joblib\n",
    "\n",
    "def calculate_itt_weights(trial):\n",
    "    # Work with the full dataset (725 observations)\n",
    "    data = trial[\"data\"].copy()\n",
    "    data = data.sort_values([\"id\", \"period\"])\n",
    "    data[\"prev_treatment\"] = data.groupby(\"id\")[\"treatment\"].shift(1).fillna(0)\n",
    "    data[\"not_censored\"] = 1 - data[\"censored\"]\n",
    "    \n",
    "    # Model n: P(censor_event = 0 | X)\n",
    "    formula_n = \"not_censored ~ x2\"\n",
    "    model_n = smf.logit(formula=formula_n, data=data).fit(disp=0)\n",
    "    trial[\"fitted_itt_censor_numerator\"] = model_n\n",
    "    \n",
    "    # Model d0: P(censor_event = 0 | X, previous treatment = 0)\n",
    "    subset_d0 = data[data[\"prev_treatment\"] == 0]\n",
    "    formula_d = \"not_censored ~ x2 + x1\"\n",
    "    model_d0 = smf.logit(formula=formula_d, data=subset_d0).fit(disp=0)\n",
    "    trial[\"fitted_itt_censor_denominator_d0\"] = model_d0\n",
    "    \n",
    "    # Model d1: P(censor_event = 0 | X, previous treatment = 1)\n",
    "    subset_d1 = data[data[\"prev_treatment\"] == 1]\n",
    "    model_d1 = smf.logit(formula=formula_d, data=subset_d1).fit(disp=0)\n",
    "    trial[\"fitted_itt_censor_denominator_d1\"] = model_d1\n",
    "    \n",
    "    # Compute predicted probabilities for all observations\n",
    "    data[\"pred_num\"] = model_n.predict(data)\n",
    "    data[\"pred_den\"] = 0.0\n",
    "    idx0 = data[\"prev_treatment\"] == 0\n",
    "    idx1 = data[\"prev_treatment\"] == 1\n",
    "    data.loc[idx0, \"pred_den\"] = model_d0.predict(data.loc[idx0])\n",
    "    data.loc[idx1, \"pred_den\"] = model_d1.predict(data.loc[idx1])\n",
    "    \n",
    "    # Calculate weight as the ratio of predicted numerator to denominator\n",
    "    data[\"weight\"] = data[\"pred_num\"] / data[\"pred_den\"]\n",
    "    \n",
    "    # Save the full dataset with calculated weights\n",
    "    trial[\"data_with_weights\"] = data.copy()\n",
    "    \n",
    "    return trial\n",
    "\n",
    "# --- Compute ITT weights ---\n",
    "trial_itt = calculate_itt_weights(trial_itt)\n",
    "\n",
    "# --- Define the path to save the CSV file ---\n",
    "data_folder = r\"C:\\Github\\Assignment_2_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\"\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "csv_path = os.path.join(data_folder, \"trial_itt_data_with_weights.csv\")\n",
    "\n",
    "# --- Save the data with weights ---\n",
    "trial_itt[\"data_with_weights\"].to_csv(csv_path, index=False)\n",
    "print(\"Stored ITT data with calculated weights to CSV file at:\")\n",
    "print(csv_path)\n",
    "\n",
    "# --- Print the fitted models' summaries ---\n",
    "print(\"\\nModel n (Numerator) Summary:\")\n",
    "print(trial_itt[\"fitted_itt_censor_numerator\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nModel d0 (Denom. for prev_treatment = 0) Summary:\")\n",
    "print(trial_itt[\"fitted_itt_censor_denominator_d0\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nModel d1 (Denom. for prev_treatment = 1) Summary:\")\n",
    "print(trial_itt[\"fitted_itt_censor_denominator_d1\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_pp_censor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trial\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# --- Compute PP weights ---\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mtrial_pp_censor\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimand\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPP\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m trial_pp_censor[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(trial_pp_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformative_censor_models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(trial_pp_censor[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trial_pp_censor' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import joblib\n",
    "\n",
    "def calculate_pp_informative_weights_updated(trial):\n",
    "    data = trial[\"data\"].copy()\n",
    "    data = data.sort_values([\"id\", \"period\"])\n",
    "    data[\"prev_treatment\"] = data.groupby(\"id\")[\"treatment\"].shift(1).fillna(0)\n",
    "    data[\"not_censored\"] = 1 - data[\"censored\"]\n",
    "    \n",
    "    # Model n0: P(censor_event = 0 | X, previous treatment = 0) for numerator\n",
    "    subset0 = data[data[\"prev_treatment\"] == 0]\n",
    "    model_n0 = smf.logit(\"not_censored ~ x2\", data=subset0).fit(disp=0)\n",
    "    trial[\"fitted_pp_censor_numerator_n0\"] = model_n0\n",
    "\n",
    "    # Model n1: P(censor_event = 0 | X, previous treatment = 1) for numerator\n",
    "    subset1 = data[data[\"prev_treatment\"] == 1]\n",
    "    model_n1 = smf.logit(\"not_censored ~ x2\", data=subset1).fit(disp=0)\n",
    "    trial[\"fitted_pp_censor_numerator_n1\"] = model_n1\n",
    "\n",
    "    # Model d0: P(censor_event = 0 | X, previous treatment = 0) for denominator\n",
    "    model_d0 = smf.logit(\"not_censored ~ x2 + x1\", data=subset0).fit(disp=0)\n",
    "    trial[\"fitted_pp_censor_denominator_d0\"] = model_d0\n",
    "\n",
    "    # Model d1: P(censor_event = 0 | X, previous treatment = 1) for denominator\n",
    "    model_d1 = smf.logit(\"not_censored ~ x2 + x1\", data=subset1).fit(disp=0)\n",
    "    trial[\"fitted_pp_censor_denominator_d1\"] = model_d1\n",
    "\n",
    "    # Compute predicted probabilities\n",
    "    data[\"pred_num\"] = 0.0\n",
    "    data[\"pred_den\"] = 0.0\n",
    "    idx0 = data[\"prev_treatment\"] == 0\n",
    "    idx1 = data[\"prev_treatment\"] == 1\n",
    "\n",
    "    data.loc[idx0, \"pred_num\"] = model_n0.predict(data.loc[idx0])\n",
    "    data.loc[idx1, \"pred_num\"] = model_n1.predict(data.loc[idx1])\n",
    "    data.loc[idx0, \"pred_den\"] = model_d0.predict(data.loc[idx0])\n",
    "    data.loc[idx1, \"pred_den\"] = model_d1.predict(data.loc[idx1])\n",
    "\n",
    "    # Calculate weight as the ratio of numerator to denominator predictions\n",
    "    data[\"weight\"] = data[\"pred_num\"] / data[\"pred_den\"]\n",
    "\n",
    "    # Save the full dataset with calculated weights\n",
    "    trial[\"data_with_weights\"] = data.copy()\n",
    "\n",
    "    return trial\n",
    "\n",
    "# --- Compute PP weights ---\n",
    "trial_pp_censor[\"estimand\"] = \"PP\"\n",
    "trial_pp_censor[\"save_dir\"] = os.path.join(trial_pp_dir, \"informative_censor_models\")\n",
    "os.makedirs(trial_pp_censor[\"save_dir\"], exist_ok=True)\n",
    "trial_pp_censor = calculate_pp_informative_weights_updated(trial_pp_censor)\n",
    "\n",
    "# --- Define the path to save the CSV file ---\n",
    "data_folder = r\"C:\\Github\\Assignment_2_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\"\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "csv_path = os.path.join(data_folder, \"trial_pp_data_with_weights.csv\")\n",
    "\n",
    "# --- Save the data with weights ---\n",
    "trial_pp_censor[\"data_with_weights\"].to_csv(csv_path, index=False)\n",
    "print(\"Stored PP data with calculated weights to CSV file at:\")\n",
    "print(csv_path)\n",
    "\n",
    "# --- Print the fitted models' summaries ---\n",
    "print(\"\\nPP model n0 (Numerator, prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_numerator_n0\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nPP model n1 (Numerator, prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_numerator_n1\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nPP model d0 (Denominator, prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_denominator_d0\"].summary2().as_text())\n",
    "\n",
    "print(\"\\nPP model d1 (Denominator, prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_censor[\"fitted_pp_censor_denominator_d1\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment Switching Weight Calculation\n",
    "\n",
    "In this section, we calculate the treatment switching weights for the Per-protocol (PP) analysis.\n",
    "We train four logistic regression models:\n",
    "- model n1: P(treatment = 1 | previous treatment = 1) for numerator.\n",
    "- model d1: P(treatment = 1 | previous treatment = 1) for denominator.\n",
    "- model n0: P(treatment = 1 | previous treatment = 0) for numerator.\n",
    "- model d0: P(treatment = 1 | previous treatment = 0) for denominator.\n",
    "\n",
    "The weights are calculated as follows:\n",
    "- For observations with previous treatment = 1:\n",
    "    weight = (predicted probability from model n1) / (predicted probability from model d1)\n",
    "- For observations with previous treatment = 0:\n",
    "    weight = (predicted probability from model n0) / (predicted probability from model d0)\n",
    "\n",
    "Logistic regression is used to estimate these probabilities and the models are saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_pp_switch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 48\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trial\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Usage example for trial PP:\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Ensure trial_pp has an assigned save_dir (e.g., within trial_pp_dir)\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mtrial_pp_switch\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimand\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPP\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m trial_pp_switch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(trial_pp_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswitch_models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(trial_pp_switch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trial_pp_switch' is not defined"
     ]
    }
   ],
   "source": [
    "# New Code Cell: Calculate Treatment Switching Weights using Logistic Regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def calculate_pp_switch_weights(trial):\n",
    "    # Ensure trial is PP and prepare data\n",
    "    data = trial[\"data\"].copy()\n",
    "    data = data.sort_values([\"id\", \"period\"])\n",
    "    data[\"prev_treatment\"] = data.groupby(\"id\")[\"treatment\"].shift(1).fillna(0)\n",
    "    \n",
    "    # Model n1: P(treatment = 1 | previous treatment = 1)\n",
    "    subset_n1 = data[data[\"prev_treatment\"] == 1]\n",
    "    formula_n1 = \"treatment ~ age\"\n",
    "    model_n1 = smf.logit(formula=formula_n1, data=subset_n1).fit(disp=0)\n",
    "    save_path_n1 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_num_model_n1.pkl\")\n",
    "    joblib.dump(model_n1, save_path_n1)\n",
    "    trial[\"fitted_pp_switch_numerator_n1\"] = model_n1\n",
    "    \n",
    "    # Model d1: P(treatment = 1 | previous treatment = 1)\n",
    "    formula_d1 = \"treatment ~ age + x1 + x3\"\n",
    "    model_d1 = smf.logit(formula=formula_d1, data=subset_n1).fit(disp=0)\n",
    "    save_path_d1 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_den_model_d1.pkl\")\n",
    "    joblib.dump(model_d1, save_path_d1)\n",
    "    trial[\"fitted_pp_switch_denominator_d1\"] = model_d1\n",
    "    \n",
    "    # Model n0: P(treatment = 1 | previous treatment = 0)\n",
    "    subset_n0 = data[data[\"prev_treatment\"] == 0]\n",
    "    formula_n0 = \"treatment ~ age\"\n",
    "    model_n0 = smf.logit(formula=formula_n0, data=subset_n0).fit(disp=0)\n",
    "    save_path_n0 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_num_model_n0.pkl\")\n",
    "    joblib.dump(model_n0, save_path_n0)\n",
    "    trial[\"fitted_pp_switch_numerator_n0\"] = model_n0\n",
    "    \n",
    "    # Model d0: P(treatment = 1 | previous treatment = 0)\n",
    "    formula_d0 = \"treatment ~ age + x1 + x3\"\n",
    "    model_d0 = smf.logit(formula=formula_d0, data=subset_n0).fit(disp=0)\n",
    "    save_path_d0 = os.path.join(trial.get(\"save_dir\", \"\"), \"pp_switch_den_model_d0.pkl\")\n",
    "    joblib.dump(model_d0, save_path_d0)\n",
    "    trial[\"fitted_pp_switch_denominator_d0\"] = model_d0\n",
    "    \n",
    "    return trial\n",
    "\n",
    "# Usage example for trial PP:\n",
    "# Ensure trial_pp has an assigned save_dir (e.g., within trial_pp_dir)\n",
    "trial_pp_switch[\"estimand\"] = \"PP\"\n",
    "trial_pp_switch[\"save_dir\"] = os.path.join(trial_pp_dir, \"switch_models\")\n",
    "os.makedirs(trial_pp_switch[\"save_dir\"], exist_ok=True)\n",
    "trial_pp_switch = calculate_pp_switch_weights(trial_pp_switch)\n",
    "\n",
    "# To verify, you can print summaries:\n",
    "print(\"Model n1 (Numerator for prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_numerator_n1\"].summary2().as_text())\n",
    "print(\"\\nModel d1 (Denom. for prev_treatment = 1) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_denominator_d1\"].summary2().as_text())\n",
    "print(\"\\nModel n0 (Numerator for prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_numerator_n0\"].summary2().as_text())\n",
    "print(\"\\nModel d0 (Denom. for prev_treatment = 0) Summary:\")\n",
    "print(trial_pp_switch[\"fitted_pp_switch_denominator_d0\"].summary2().as_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Outcome Model\n",
    "Now we can specify the outcome model. Here we can include adjustment terms for any variables in the dataset. The numerator terms from the stabilised weight models are automatically included in the outcome model formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_pp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trial\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# --- Process outcome data for PP and ITT using the data that already has calculated weights\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m trial_pp \u001b[38;5;241m=\u001b[39m process_outcome_data(\u001b[43mtrial_pp\u001b[49m)\n\u001b[0;32m     16\u001b[0m trial_itt \u001b[38;5;241m=\u001b[39m process_outcome_data(trial_itt, adjustment_terms\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m + x2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# --- Print data structure summaries\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trial_pp' is not defined"
     ]
    }
   ],
   "source": [
    "def process_outcome_data(trial, adjustment_terms=\"\"):\n",
    "    # Use data with weights if available; otherwise, use the original data.\n",
    "    data = trial.get(\"data_with_weights\", trial[\"data\"]).copy()\n",
    "    \n",
    "    # Ensure assigned_treatment exists in the dataset (same as treatment)\n",
    "    data[\"assigned_treatment\"] = data.get(\"assigned_treatment\", data[\"treatment\"])\n",
    "    \n",
    "    # Store adjustment terms in trial dictionary for later use\n",
    "    trial[\"processed_data\"] = data\n",
    "    trial[\"adjustment_terms\"] = adjustment_terms\n",
    "    \n",
    "    return trial\n",
    "\n",
    "# --- Process outcome data for PP and ITT using the data that already has calculated weights\n",
    "trial_pp = process_outcome_data(trial_pp)\n",
    "trial_itt = process_outcome_data(trial_itt, adjustment_terms=\" + x2\")\n",
    "\n",
    "# --- Print data structure summaries\n",
    "print(\"PP Processed Data Sample:\")\n",
    "print(trial_pp[\"processed_data\"].head())\n",
    "\n",
    "print(\"\\nITT Processed Data Sample:\")\n",
    "print(trial_itt[\"processed_data\"].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand Trials\n",
    "\n",
    "We prepare to create the dataset that includes the sequence of target trials. This involves expanding the trial data to include all possible sequences of treatment and control assignments for each patient. \n",
    "\n",
    "We use the `set_expansion_options` function to configure the expansion process. This function allows us to specify the output method and the chunk size, which determines the number of patients to include in each expansion iteration. \n",
    "\n",
    "For both the Per-protocol (PP) and Intention-to-treat (ITT) analyses, we set the output to a dummy function `save_to_datatable()` and the chunk size to 500 patients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_pp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(expanded_chunks, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# --- Expand both PP and ITT datasets ---\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m trial_pp_expanded \u001b[38;5;241m=\u001b[39m expand_trials(\u001b[43mtrial_pp\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_data\u001b[39m\u001b[38;5;124m\"\u001b[39m], chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m     34\u001b[0m trial_itt_expanded \u001b[38;5;241m=\u001b[39m expand_trials(trial_itt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_data\u001b[39m\u001b[38;5;124m\"\u001b[39m], chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# --- Define the path to save the CSV files ---\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trial_pp' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def expand_trials(data, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Expands trial data in chunks, repeating each row for multiple trial periods.\n",
    "    \"\"\"\n",
    "    expanded_chunks = []\n",
    "\n",
    "    # Define trial periods (consistent with tutorial defaults)\n",
    "    periods = np.arange(11)  # Includes periods 0 to 10\n",
    "\n",
    "    # Process data in chunks\n",
    "    for start in range(0, len(data), chunk_size):\n",
    "        chunk = data.iloc[start:start + chunk_size].copy()\n",
    "\n",
    "        expanded_chunk = (\n",
    "            chunk.loc[chunk.index.repeat(len(periods))]  # Repeat rows for each period\n",
    "            .assign(trial_period=np.tile(periods, len(chunk)))  # Assign trial periods\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Assign follow-up time (same as trial_period)\n",
    "        expanded_chunk[\"followup_time\"] = expanded_chunk[\"trial_period\"]\n",
    "\n",
    "        expanded_chunks.append(expanded_chunk)\n",
    "\n",
    "    # Concatenate all expanded chunks\n",
    "    return pd.concat(expanded_chunks, ignore_index=True)\n",
    "\n",
    "# --- Expand both PP and ITT datasets ---\n",
    "trial_pp_expanded = expand_trials(trial_pp[\"processed_data\"], chunk_size=500)\n",
    "trial_itt_expanded = expand_trials(trial_itt[\"processed_data\"], chunk_size=500)\n",
    "\n",
    "# --- Define the path to save the CSV files ---\n",
    "data_folder = r\"C:\\Github\\Assignment_2_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\"\n",
    "os.makedirs(data_folder, exist_ok=True)  # Ensure the folder exists\n",
    "\n",
    "# --- Save expanded datasets ---\n",
    "csv_path_pp = os.path.join(data_folder, \"trial_pp_expanded.csv\")\n",
    "csv_path_itt = os.path.join(data_folder, \"trial_itt_expanded.csv\")\n",
    "\n",
    "trial_pp_expanded.to_csv(csv_path_pp, index=False)\n",
    "trial_itt_expanded.to_csv(csv_path_itt, index=False)\n",
    "\n",
    "print(\"Stored PP expanded data at:\", csv_path_pp)\n",
    "print(\"Stored ITT expanded data at:\", csv_path_itt)\n",
    "\n",
    "# --- Check sample output ---\n",
    "print(\"\\nSample from PP Expanded Data:\")\n",
    "print(trial_pp_expanded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load or Sample from Expanded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_itt_expanded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sampled_data\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# --- Load and sample from expanded ITT data ---\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m trial_itt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpanded_data\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrial_itt_expanded\u001b[49m  \u001b[38;5;66;03m# Ensure expanded data is stored in trial_itt\u001b[39;00m\n\u001b[0;32m     25\u001b[0m sampled_itt_data \u001b[38;5;241m=\u001b[39m load_expanded_data(trial_itt, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m, p_control\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# --- Define the path to save the sampled data ---\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trial_itt_expanded' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_expanded_data(trial, seed=1234, p_control=0.5, period_range=None):\n",
    "\n",
    "    np.random.seed(seed)  # Set seed for reproducibility\n",
    "\n",
    "    # Load the expanded data\n",
    "    data = trial[\"expanded_data\"].copy()\n",
    "\n",
    "    # Apply period filtering if specified\n",
    "    if period_range:\n",
    "        min_period, max_period = period_range\n",
    "        data = data[(data[\"trial_period\"] >= min_period) & (data[\"trial_period\"] <= max_period)]\n",
    "\n",
    "    # Apply p_control sampling: Keep all outcome == 1, sample outcome == 0\n",
    "    outcome_0_mask = (data[\"outcome\"] == 0)\n",
    "    sampled_data = data.loc[~outcome_0_mask | (np.random.rand(len(data)) < p_control)]\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "# --- Load and sample from expanded ITT data ---\n",
    "trial_itt[\"expanded_data\"] = trial_itt_expanded  # Ensure expanded data is stored in trial_itt\n",
    "sampled_itt_data = load_expanded_data(trial_itt, seed=1234, p_control=0.5)\n",
    "\n",
    "# --- Define the path to save the sampled data ---\n",
    "data_folder = r\"C:\\Github\\Assignment_2_Data_Analytics\\Assignment_1_Clustering_Data_Analytics\\Data\"\n",
    "os.makedirs(data_folder, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "csv_path_sampled = os.path.join(data_folder, \"trial_itt_load.csv\")  # Correct variable name\n",
    "\n",
    "# --- Save the sampled dataset ---\n",
    "sampled_itt_data.to_csv(csv_path_sampled, index=False)\n",
    "\n",
    "print(\"Stored sampled ITT data at:\", csv_path_sampled)\n",
    "\n",
    "# --- Check sample output ---\n",
    "print(\"\\nSample from ITT Sampled Data:\")\n",
    "print(sampled_itt_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fit Marginal Structural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampled_itt_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# --- Fit MSM on ITT dataset ---\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m trial_itt_msm \u001b[38;5;241m=\u001b[39m fit_msm(\u001b[43msampled_itt_data\u001b[49m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# --- Print summary of fitted MSM model ---\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMarginal Structural Model (MSM) Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sampled_itt_data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def fit_msm(data, weight_cols=[\"weight\", \"sample_weight\"], modify_weights=True):\n",
    "\n",
    "    # Ensure weights exist\n",
    "    for col in weight_cols:\n",
    "        if col not in data.columns:\n",
    "            data[col] = 1.0  # Default to 1 if missing\n",
    "\n",
    "    # Combine weights multiplicatively\n",
    "    data[\"final_weight\"] = data[weight_cols].prod(axis=1)\n",
    "\n",
    "    # Winsorization: Cap extreme weights at the 99th percentile\n",
    "    if modify_weights:\n",
    "        q99 = data[\"final_weight\"].quantile(0.99)\n",
    "        data[\"final_weight\"] = np.minimum(data[\"final_weight\"], q99)\n",
    "\n",
    "    # Fit logistic regression model for the outcome (example model)\n",
    "    formula = \"outcome ~ treatment + trial_period + x1 + x2\"\n",
    "    model = sm.GLM.from_formula(formula, data, \n",
    "                                family=sm.families.Binomial(), \n",
    "                                freq_weights=data[\"final_weight\"]).fit()\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Fit MSM on ITT dataset ---\n",
    "trial_itt_msm = fit_msm(sampled_itt_data)\n",
    "\n",
    "# --- Print summary of fitted MSM model ---\n",
    "print(\"\\nMarginal Structural Model (MSM) Summary:\")\n",
    "print(trial_itt_msm.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_itt_msm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 60\u001b[0m\n\u001b[0;32m     55\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Assume trial_itt_msm is the fitted marginal structural model produced by fit_msm(),\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# and sampled_itt_data is the correctly loaded dataset with follow-up time information.\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m predict_survival(\u001b[43mtrial_itt_msm\u001b[49m, sampled_itt_data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trial_itt_msm' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_survival(model, sampled_itt_data, predict_times=np.arange(0, 11)):\n",
    "    \"\"\"\n",
    "    Predicts survival probabilities for control and treated groups over follow-up time.\n",
    "    Uses `sampled_itt_data`, which contains follow-up time information.\n",
    "    \n",
    "    The model should be a fitted statsmodels regression (e.g., from fit_msm()).\n",
    "    \"\"\"\n",
    "\n",
    "    # Verify required predictors exist.\n",
    "    required_cols = [\"treatment\", \"followup_time\", \"x1\", \"x2\"]\n",
    "    missing_cols = [col for col in required_cols if col not in sampled_itt_data.columns]\n",
    "    if missing_cols:\n",
    "        raise KeyError(f\"Missing required columns in sampled_itt_data: {missing_cols}\")\n",
    "\n",
    "    # Prepare prediction datasets for control and treated groups.\n",
    "    data_control = sampled_itt_data.copy()\n",
    "    data_control[\"treatment\"] = 0  # Control group\n",
    "\n",
    "    data_treated = sampled_itt_data.copy()\n",
    "    data_treated[\"treatment\"] = 1  # Treated group\n",
    "\n",
    "    # Add constant term for prediction.\n",
    "    predictors = [\"treatment\", \"followup_time\", \"x1\", \"x2\"]\n",
    "    data_control_const = sm.add_constant(data_control[predictors])\n",
    "    data_treated_const = sm.add_constant(data_treated[predictors])\n",
    "\n",
    "    # Compute predicted risks/probabilities using the fitted model.\n",
    "    preds_control = model.predict(data_control_const)\n",
    "    preds_treated = model.predict(data_treated_const)\n",
    "\n",
    "    # Compute survival probabilities as 1 minus predicted risk.\n",
    "    survival_control = 1 - preds_control.groupby(data_control[\"followup_time\"]).mean()\n",
    "    survival_treated = 1 - preds_treated.groupby(data_treated[\"followup_time\"]).mean()\n",
    "\n",
    "    # Compute survival difference between treated and control.\n",
    "    survival_diff = survival_treated - survival_control\n",
    "\n",
    "    # Compute an approximate 95% confidence interval using the normal approximation.\n",
    "    ci = 1.96 * survival_diff.std()  # Element-wise standard deviation\n",
    "\n",
    "    # Plot results.\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(survival_diff.index, survival_diff, label=\"Survival Difference\", color=\"blue\")\n",
    "    plt.fill_between(survival_diff.index, survival_diff - ci, survival_diff + ci, \n",
    "                     color=\"red\", alpha=0.2, label=\"95% CI\")\n",
    "    plt.xlabel(\"Follow-up Time\")  # Updated x-axis label\n",
    "    plt.ylabel(\"Survival Difference\")\n",
    "    plt.title(\"Predicted Survival Difference Over Follow-up Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assume trial_itt_msm is the fitted marginal structural model produced by fit_msm(),\n",
    "# and sampled_itt_data is the correctly loaded dataset with follow-up time information.\n",
    "predict_survival(trial_itt_msm, sampled_itt_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Insights on TTEv2 - Clustering and Treatment Effects**\n",
    "1. **Clustering Approach:**\n",
    "   - K-Means clustering is used with **3 clusters**.\n",
    "   - Features used for clustering: `x1`, `x2`, `x3`, `x4`, and `age`.\n",
    "   - The dataset is modified to include a `Cluster_ID` column.\n",
    "\n",
    "2. **Treatment Effects Across Clusters:**\n",
    "   - The model estimates treatment effects within each cluster using **OLS regression**.\n",
    "   - The regression outputs suggest that the **effect of treatment varies across clusters**.\n",
    "   - Differences in coefficients and significance levels indicate that **some clusters respond better to treatment than others**.\n",
    "\n",
    "3. **Do Clusters Respond Differently to Treatment?**\n",
    "   - Yes, the OLS results show **variation in treatment effects** across clusters.\n",
    "   - Some clusters may have a **stronger or weaker** response to treatment.\n",
    "   - This suggests that **targeted interventions** may be more effective than a one-size-fits-all approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
